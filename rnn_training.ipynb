{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "_wHfCDyzceEl",
        "4iC21bopceEl",
        "azltQy-gceEl",
        "9YyL91CuceEl",
        "s9uKOvbqceEl",
        "942mjdQHceEm"
      ]
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PlatonovGen - Character-Level LSTM\n",
        "\n",
        "Training a neural recurrent network for generating Andrei Platonov-style texts."
      ],
      "metadata": {
        "id": "1W8R8WgZceEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "sqUOE2flceEl",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:33.348992Z",
          "iopub.execute_input": "2024-02-03T17:52:33.349408Z",
          "iopub.status.idle": "2024-02-03T17:52:36.323677Z",
          "shell.execute_reply.started": "2024-02-03T17:52:33.349378Z",
          "shell.execute_reply": "2024-02-03T17:52:36.322643Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "\n",
        "Let's load the dataset Platonov8, consisting of the 8-volume collection of the writer's works"
      ],
      "metadata": {
        "id": "_wHfCDyzceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/moxeeem/PlatonovGen/main/Platonov8.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXAxa--jPUSd",
        "outputId": "bec2e1ef-7c29-4894-dfbc-8e1e0be481d5",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:36.325715Z",
          "iopub.execute_input": "2024-02-03T17:52:36.326224Z",
          "iopub.status.idle": "2024-02-03T17:52:38.310436Z",
          "shell.execute_reply.started": "2024-02-03T17:52:36.326189Z",
          "shell.execute_reply": "2024-02-03T17:52:38.309312Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-03 19:45:31--  https://raw.githubusercontent.com/moxeeem/PlatonovGen/main/Platonov8.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12291197 (12M) [text/plain]\n",
            "Saving to: ‘Platonov8.txt’\n",
            "\n",
            "Platonov8.txt       100%[===================>]  11.72M  66.3MB/s    in 0.2s    \n",
            "\n",
            "2024-02-03 19:45:35 (66.3 MB/s) - ‘Platonov8.txt’ saved [12291197/12291197]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Platonov8.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "b34kfqIOceEl",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:38.311861Z",
          "iopub.execute_input": "2024-02-03T17:52:38.31215Z",
          "iopub.status.idle": "2024-02-03T17:52:38.393223Z",
          "shell.execute_reply.started": "2024-02-03T17:52:38.312124Z",
          "shell.execute_reply": "2024-02-03T17:52:38.392243Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at first 100 characters:"
      ],
      "metadata": {
        "id": "Jp1Ljc4mceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "id": "7VctmLQfceEl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "54174455-4fdf-4cf6-bfc6-9b8fba8ccd21",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:38.395896Z",
          "iopub.execute_input": "2024-02-03T17:52:38.396576Z",
          "iopub.status.idle": "2024-02-03T17:52:38.403623Z",
          "shell.execute_reply.started": "2024-02-03T17:52:38.396541Z",
          "shell.execute_reply": "2024-02-03T17:52:38.402725Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Издревле и повсесюдно все старики спят. Спят так, что пузыри от уст отскакивают и одиноко мокнет поз'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Let's create a couple dictionaries to convert the characters to and from integers."
      ],
      "metadata": {
        "id": "4iC21bopceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "tYVlmnxLceEl",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:38.404914Z",
          "iopub.execute_input": "2024-02-03T17:52:38.405504Z",
          "iopub.status.idle": "2024-02-03T17:52:40.167466Z",
          "shell.execute_reply.started": "2024-02-03T17:52:38.405455Z",
          "shell.execute_reply": "2024-02-03T17:52:40.166613Z"
        },
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the characters are encoded with integers"
      ],
      "metadata": {
        "id": "oJIzwzSwceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded[:100]"
      ],
      "metadata": {
        "id": "WK1MYr_9ceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1e0380-fd9e-4b48-9f2d-0b74ed00730e",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.168626Z",
          "iopub.execute_input": "2024-02-03T17:52:40.16892Z",
          "iopub.status.idle": "2024-02-03T17:52:40.175878Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.168894Z",
          "shell.execute_reply": "2024-02-03T17:52:40.174861Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([110,  80, 134,  99,  56,  39, 128,  56,  47,  72,  47, 140,  95,\n",
              "        39,  44,  56,  44, 108, 134,  85,  95,  47,  39,  44,  56,  47,\n",
              "        44, 151,  96,  99,  72,  53,  72,  47,  44, 140,  14, 151, 125,\n",
              "        47, 124, 140,  14, 151,  47, 151,  96,  53,  15,  47,   0, 151,\n",
              "        95,  47, 140,   5,  80,  73,  99,  72,  47,  95, 151,  47,   5,\n",
              "        44, 151,  47,  95, 151,  44,  53,  96,  53,  72,  39,  96, 108,\n",
              "       151,  47,  72,  47,  95, 134,  72,  85,  95,  53,  95,  47, 103,\n",
              "        95,  53,  85,  56, 151,  47, 140,  95,  80])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "LSTM expects **one-hot encoded** input, which means that each character is converted to an integer and then converted to a column vector where only its corresponding integer index will have a value of 1 and the rest of the vector will be filled with zeros. Let's create a function for this."
      ],
      "metadata": {
        "id": "azltQy-gceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "OnahALhiceEl",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.177215Z",
          "iopub.execute_input": "2024-02-03T17:52:40.17752Z",
          "iopub.status.idle": "2024-02-03T17:52:40.186006Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.17749Z",
          "shell.execute_reply": "2024-02-03T17:52:40.184982Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "metadata": {
        "id": "L3lTdLKfceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b49cf3-6cda-4b3c-812f-c589ae53d77e",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.187205Z",
          "iopub.execute_input": "2024-02-03T17:52:40.187547Z",
          "iopub.status.idle": "2024-02-03T17:52:40.195838Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.187522Z",
          "shell.execute_reply": "2024-02-03T17:52:40.194905Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Training Mini-Batches"
      ],
      "metadata": {
        "id": "9YyL91CuceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    Generates batches from encoded sequence.\n",
        "    :param int_words: tensor of ints, which represents the text; shape: [batch_size, -1]\n",
        "    :param batch_size: number of sequences per batch\n",
        "    :param seq_length: number of encoded chars in a sequence\n",
        "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
        "    \"\"\"\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "FLBOH8ZQUl7B",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.197033Z",
          "iopub.execute_input": "2024-02-03T17:52:40.197324Z",
          "iopub.status.idle": "2024-02-03T17:52:40.206328Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.1973Z",
          "shell.execute_reply": "2024-02-03T17:52:40.205413Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n",
        "\n",
        "Let's create some datasets, and check what happens when we create batches."
      ],
      "metadata": {
        "id": "s9uKOvbqceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "metadata": {
        "id": "qtKlLXi1ceEl",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.209859Z",
          "iopub.execute_input": "2024-02-03T17:52:40.210144Z",
          "iopub.status.idle": "2024-02-03T17:52:40.217346Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.210122Z",
          "shell.execute_reply": "2024-02-03T17:52:40.216514Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print(\"x\\n\", x[:10, :10])\n",
        "print(\"\\ny\\n\", y[:10, :10])"
      ],
      "metadata": {
        "id": "Rg5MUTqqceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e52b6a-d97b-4883-f29e-c331fb1f9a8c",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.218578Z",
          "iopub.execute_input": "2024-02-03T17:52:40.219325Z",
          "iopub.status.idle": "2024-02-03T17:52:40.228133Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.219299Z",
          "shell.execute_reply": "2024-02-03T17:52:40.227223Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[110  80 134  99  56  39 128  56  47  72]\n",
            " [ 47 151  96  53  47  85  72  47  53  47]\n",
            " [ 56 128  95  39  56  53  96  47 144  47]\n",
            " [116  56  47  85  56  47   5  44 140  56]\n",
            " [ 47  45  95  99  95 134  56  47  85  96]\n",
            " [ 72  56  47  80  96  47  39  99  56 103]\n",
            " [ 47 129  96  80  47  72 128  72  47 134]\n",
            " [ 85  85  95  47  85  96  99  95 134  85]]\n",
            "\n",
            "y\n",
            " [[ 80 134  99  56  39 128  56  47  72  47]\n",
            " [151  96  53  47  85  72  47  53  47   0]\n",
            " [128  95  39  56  53  96  47 144  47  39]\n",
            " [ 56  47  85  56  47   5  44 140  56 128]\n",
            " [ 45  95  99  95 134  56  47  85  96  47]\n",
            " [ 56  47  80  96  47  39  99  56 103  14]\n",
            " [129  96  80  47  72 128  72  47 134  39]\n",
            " [ 85  95  47  85  96  99  95 134  85  95]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Network Setting\n"
      ],
      "metadata": {
        "id": "Jouxv0L2ceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ],
      "metadata": {
        "id": "HlTnDntHceEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ce824f-84c2-4d93-ebac-c05f83818b0b",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.229293Z",
          "iopub.execute_input": "2024-02-03T17:52:40.229631Z",
          "iopub.status.idle": "2024-02-03T17:52:40.263658Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.229604Z",
          "shell.execute_reply": "2024-02-03T17:52:40.262638Z"
        },
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "      #drop_prob=0.5\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "VPq1EA38rBqn",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.265167Z",
          "iopub.execute_input": "2024-02-03T17:52:40.265518Z",
          "iopub.status.idle": "2024-02-03T17:52:40.279338Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.265466Z",
          "shell.execute_reply": "2024-02-03T17:52:40.278259Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Training"
      ],
      "metadata": {
        "id": "5IrBRlEPceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    \"\"\"Training a network\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "\n",
        "    net: CharRNN network\n",
        "    data: text data to train the network\n",
        "    epochs: Number of epochs to train\n",
        "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "    seq_length: Number of character steps per mini-batch\n",
        "    lr: learning rate\n",
        "    clip: gradient clipping\n",
        "    val_frac: Fraction of data to hold out for validation\n",
        "    print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )\n",
        "\n",
        "\n",
        "        # Save the model every 10 epochs\n",
        "        if (e + 1) % 10 == 0:\n",
        "            checkpoint = {\n",
        "                \"n_hidden\": net.n_hidden,\n",
        "                \"n_layers\": net.n_layers,\n",
        "                \"state_dict\": net.state_dict(),\n",
        "                \"tokens\": net.chars,\n",
        "            }\n",
        "            with open(f\"checkpoint_{e + 1}_epoch.net\", \"wb\") as f:\n",
        "                torch.save(checkpoint, f)"
      ],
      "metadata": {
        "id": "lv8VkRI0ceEl",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.280761Z",
          "iopub.execute_input": "2024-02-03T17:52:40.281069Z",
          "iopub.status.idle": "2024-02-03T17:52:40.301434Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.281044Z",
          "shell.execute_reply": "2024-02-03T17:52:40.300588Z"
        },
        "trusted": true
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Definition\n"
      ],
      "metadata": {
        "id": "Gt0q4KGEceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden = 1024\n",
        "n_layers = 3\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "ykMcIloEr3G7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc8e9b0-dfcb-4f29-d04b-9b6bef7db9fc",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.302629Z",
          "iopub.execute_input": "2024-02-03T17:52:40.302943Z",
          "iopub.status.idle": "2024-02-03T17:52:40.545459Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.302917Z",
          "shell.execute_reply": "2024-02-03T17:52:40.544513Z"
        },
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(152, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=152, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters Setting and Training"
      ],
      "metadata": {
        "id": "XHy6mECuceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=10,\n",
        ")\n"
      ],
      "metadata": {
        "id": "8hTkNrWEsjgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a012fa15-eb8c-4e21-80ed-09e73745c1e1",
        "execution": {
          "iopub.status.busy": "2024-02-03T17:52:40.546726Z",
          "iopub.execute_input": "2024-02-03T17:52:40.548212Z",
          "iopub.status.idle": "2024-02-03T19:40:09.035198Z",
          "shell.execute_reply.started": "2024-02-03T17:52:40.548175Z",
          "shell.execute_reply": "2024-02-03T19:40:09.034096Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch: 1/20... Step: 10... Loss: 3.4097... Val Loss: 3.3195\nEpoch: 1/20... Step: 20... Loss: 3.3985... Val Loss: 3.3023\nEpoch: 1/20... Step: 30... Loss: 3.3815... Val Loss: 3.2940\nEpoch: 1/20... Step: 40... Loss: 3.3482... Val Loss: 3.2894\nEpoch: 1/20... Step: 50... Loss: 3.3326... Val Loss: 3.2943\nEpoch: 1/20... Step: 60... Loss: 3.3262... Val Loss: 3.2874\nEpoch: 1/20... Step: 70... Loss: 3.3388... Val Loss: 3.2895\nEpoch: 1/20... Step: 80... Loss: 3.3348... Val Loss: 3.2894\nEpoch: 1/20... Step: 90... Loss: 3.3212... Val Loss: 3.2895\nEpoch: 1/20... Step: 100... Loss: 3.3015... Val Loss: 3.2841\nEpoch: 1/20... Step: 110... Loss: 3.3196... Val Loss: 3.2876\nEpoch: 1/20... Step: 120... Loss: 3.3206... Val Loss: 3.2867\nEpoch: 1/20... Step: 130... Loss: 3.3159... Val Loss: 3.2894\nEpoch: 1/20... Step: 140... Loss: 3.3261... Val Loss: 3.2871\nEpoch: 1/20... Step: 150... Loss: 3.3041... Val Loss: 3.2839\nEpoch: 1/20... Step: 160... Loss: 3.2988... Val Loss: 3.2855\nEpoch: 1/20... Step: 170... Loss: 3.2841... Val Loss: 3.2497\nEpoch: 1/20... Step: 180... Loss: 3.2823... Val Loss: 3.1561\nEpoch: 1/20... Step: 190... Loss: 3.1552... Val Loss: 3.0640\nEpoch: 1/20... Step: 200... Loss: 3.0442... Val Loss: 3.0092\nEpoch: 1/20... Step: 210... Loss: 2.9615... Val Loss: 2.9312\nEpoch: 1/20... Step: 220... Loss: 2.9179... Val Loss: 2.9147\nEpoch: 1/20... Step: 230... Loss: 2.8768... Val Loss: 2.8422\nEpoch: 1/20... Step: 240... Loss: 2.7919... Val Loss: 2.7884\nEpoch: 1/20... Step: 250... Loss: 2.7526... Val Loss: 2.7513\nEpoch: 1/20... Step: 260... Loss: 2.6903... Val Loss: 2.7300\nEpoch: 1/20... Step: 270... Loss: 2.6873... Val Loss: 2.6832\nEpoch: 1/20... Step: 280... Loss: 2.6627... Val Loss: 2.6639\nEpoch: 1/20... Step: 290... Loss: 2.6600... Val Loss: 2.6535\nEpoch: 1/20... Step: 300... Loss: 2.6312... Val Loss: 2.6174\nEpoch: 1/20... Step: 310... Loss: 2.6198... Val Loss: 2.5951\nEpoch: 1/20... Step: 320... Loss: 2.5774... Val Loss: 2.5766\nEpoch: 1/20... Step: 330... Loss: 2.5718... Val Loss: 2.5631\nEpoch: 1/20... Step: 340... Loss: 2.5541... Val Loss: 2.5427\nEpoch: 1/20... Step: 350... Loss: 2.5430... Val Loss: 2.5217\nEpoch: 1/20... Step: 360... Loss: 2.5341... Val Loss: 2.5080\nEpoch: 1/20... Step: 370... Loss: 2.5059... Val Loss: 2.4914\nEpoch: 1/20... Step: 380... Loss: 2.4754... Val Loss: 2.4779\nEpoch: 1/20... Step: 390... Loss: 2.4840... Val Loss: 2.4623\nEpoch: 1/20... Step: 400... Loss: 2.4436... Val Loss: 2.4587\nEpoch: 1/20... Step: 410... Loss: 2.4649... Val Loss: 2.4365\nEpoch: 1/20... Step: 420... Loss: 2.4258... Val Loss: 2.4285\nEpoch: 1/20... Step: 430... Loss: 2.4227... Val Loss: 2.4086\nEpoch: 1/20... Step: 440... Loss: 2.4151... Val Loss: 2.3960\nEpoch: 1/20... Step: 450... Loss: 2.3953... Val Loss: 2.3904\nEpoch: 1/20... Step: 460... Loss: 2.4050... Val Loss: 2.3686\nEpoch: 2/20... Step: 470... Loss: 2.4335... Val Loss: 2.3573\nEpoch: 2/20... Step: 480... Loss: 2.3625... Val Loss: 2.3471\nEpoch: 2/20... Step: 490... Loss: 2.3761... Val Loss: 2.3341\nEpoch: 2/20... Step: 500... Loss: 2.3485... Val Loss: 2.3194\nEpoch: 2/20... Step: 510... Loss: 2.3367... Val Loss: 2.3065\nEpoch: 2/20... Step: 520... Loss: 2.3406... Val Loss: 2.2993\nEpoch: 2/20... Step: 530... Loss: 2.3063... Val Loss: 2.2820\nEpoch: 2/20... Step: 540... Loss: 2.3336... Val Loss: 2.2646\nEpoch: 2/20... Step: 550... Loss: 2.3066... Val Loss: 2.2556\nEpoch: 2/20... Step: 560... Loss: 2.3179... Val Loss: 2.2403\nEpoch: 2/20... Step: 570... Loss: 2.2500... Val Loss: 2.2271\nEpoch: 2/20... Step: 580... Loss: 2.2632... Val Loss: 2.2123\nEpoch: 2/20... Step: 590... Loss: 2.2348... Val Loss: 2.1942\nEpoch: 2/20... Step: 600... Loss: 2.2231... Val Loss: 2.1849\nEpoch: 2/20... Step: 610... Loss: 2.2368... Val Loss: 2.1773\nEpoch: 2/20... Step: 620... Loss: 2.2225... Val Loss: 2.1653\nEpoch: 2/20... Step: 630... Loss: 2.2096... Val Loss: 2.1502\nEpoch: 2/20... Step: 640... Loss: 2.1698... Val Loss: 2.1370\nEpoch: 2/20... Step: 650... Loss: 2.1634... Val Loss: 2.1286\nEpoch: 2/20... Step: 660... Loss: 2.1920... Val Loss: 2.1124\nEpoch: 2/20... Step: 670... Loss: 2.1878... Val Loss: 2.1017\nEpoch: 2/20... Step: 680... Loss: 2.1497... Val Loss: 2.0911\nEpoch: 2/20... Step: 690... Loss: 2.1350... Val Loss: 2.0836\nEpoch: 2/20... Step: 700... Loss: 2.1434... Val Loss: 2.0728\nEpoch: 2/20... Step: 710... Loss: 2.0916... Val Loss: 2.0603\nEpoch: 2/20... Step: 720... Loss: 2.0968... Val Loss: 2.0503\nEpoch: 2/20... Step: 730... Loss: 2.0960... Val Loss: 2.0383\nEpoch: 2/20... Step: 740... Loss: 2.0953... Val Loss: 2.0288\nEpoch: 2/20... Step: 750... Loss: 2.0715... Val Loss: 2.0179\nEpoch: 2/20... Step: 760... Loss: 2.0687... Val Loss: 2.0091\nEpoch: 2/20... Step: 770... Loss: 2.0725... Val Loss: 2.0008\nEpoch: 2/20... Step: 780... Loss: 2.0509... Val Loss: 1.9941\nEpoch: 2/20... Step: 790... Loss: 2.0650... Val Loss: 1.9873\nEpoch: 2/20... Step: 800... Loss: 2.0435... Val Loss: 1.9756\nEpoch: 2/20... Step: 810... Loss: 2.0061... Val Loss: 1.9677\nEpoch: 2/20... Step: 820... Loss: 2.0748... Val Loss: 1.9646\nEpoch: 2/20... Step: 830... Loss: 2.0057... Val Loss: 1.9580\nEpoch: 2/20... Step: 840... Loss: 1.9909... Val Loss: 1.9486\nEpoch: 2/20... Step: 850... Loss: 1.9982... Val Loss: 1.9428\nEpoch: 2/20... Step: 860... Loss: 1.9997... Val Loss: 1.9367\nEpoch: 2/20... Step: 870... Loss: 2.0125... Val Loss: 1.9362\nEpoch: 2/20... Step: 880... Loss: 1.9838... Val Loss: 1.9273\nEpoch: 2/20... Step: 890... Loss: 1.9946... Val Loss: 1.9223\nEpoch: 2/20... Step: 900... Loss: 1.9462... Val Loss: 1.9145\nEpoch: 2/20... Step: 910... Loss: 1.9202... Val Loss: 1.9073\nEpoch: 2/20... Step: 920... Loss: 1.9464... Val Loss: 1.9093\nEpoch: 2/20... Step: 930... Loss: 1.9851... Val Loss: 1.8987\nEpoch: 3/20... Step: 940... Loss: 1.9582... Val Loss: 1.8904\nEpoch: 3/20... Step: 950... Loss: 1.9165... Val Loss: 1.8928\nEpoch: 3/20... Step: 960... Loss: 1.9247... Val Loss: 1.8844\nEpoch: 3/20... Step: 970... Loss: 1.9178... Val Loss: 1.9002\nEpoch: 3/20... Step: 980... Loss: 1.9528... Val Loss: 1.8740\nEpoch: 3/20... Step: 990... Loss: 1.9692... Val Loss: 1.8879\nEpoch: 3/20... Step: 1000... Loss: 1.9271... Val Loss: 1.8812\nEpoch: 3/20... Step: 1010... Loss: 1.9264... Val Loss: 1.8704\nEpoch: 3/20... Step: 1020... Loss: 1.9179... Val Loss: 1.8532\nEpoch: 3/20... Step: 1030... Loss: 1.9127... Val Loss: 1.8497\nEpoch: 3/20... Step: 1040... Loss: 1.9375... Val Loss: 1.8565\nEpoch: 3/20... Step: 1050... Loss: 1.9114... Val Loss: 1.8570\nEpoch: 3/20... Step: 1060... Loss: 1.9425... Val Loss: 1.8349\nEpoch: 3/20... Step: 1070... Loss: 1.9116... Val Loss: 1.8330\nEpoch: 3/20... Step: 1080... Loss: 1.9169... Val Loss: 1.8394\nEpoch: 3/20... Step: 1090... Loss: 1.9278... Val Loss: 1.8231\nEpoch: 3/20... Step: 1100... Loss: 1.9304... Val Loss: 1.8291\nEpoch: 3/20... Step: 1110... Loss: 1.9158... Val Loss: 1.8377\nEpoch: 3/20... Step: 1120... Loss: 1.9366... Val Loss: 1.8159\nEpoch: 3/20... Step: 1130... Loss: 1.9608... Val Loss: 1.8054\nEpoch: 3/20... Step: 1140... Loss: 1.8625... Val Loss: 1.8125\nEpoch: 3/20... Step: 1150... Loss: 1.8106... Val Loss: 1.8034\nEpoch: 3/20... Step: 1160... Loss: 1.8344... Val Loss: 1.7956\nEpoch: 3/20... Step: 1170... Loss: 1.8453... Val Loss: 1.7933\nEpoch: 3/20... Step: 1180... Loss: 1.8289... Val Loss: 1.7895\nEpoch: 3/20... Step: 1190... Loss: 1.8269... Val Loss: 1.7819\nEpoch: 3/20... Step: 1200... Loss: 1.8115... Val Loss: 1.7791\nEpoch: 3/20... Step: 1210... Loss: 1.8283... Val Loss: 1.7750\nEpoch: 3/20... Step: 1220... Loss: 1.8126... Val Loss: 1.7693\nEpoch: 3/20... Step: 1230... Loss: 1.7964... Val Loss: 1.7632\nEpoch: 3/20... Step: 1240... Loss: 1.8014... Val Loss: 1.7625\nEpoch: 3/20... Step: 1250... Loss: 1.8161... Val Loss: 1.7562\nEpoch: 3/20... Step: 1260... Loss: 1.7890... Val Loss: 1.7572\nEpoch: 3/20... Step: 1270... Loss: 1.7576... Val Loss: 1.7588\nEpoch: 3/20... Step: 1280... Loss: 1.7747... Val Loss: 1.7555\nEpoch: 3/20... Step: 1290... Loss: 1.8078... Val Loss: 1.7518\nEpoch: 3/20... Step: 1300... Loss: 1.8002... Val Loss: 1.7522\nEpoch: 3/20... Step: 1310... Loss: 1.7753... Val Loss: 1.7466\nEpoch: 3/20... Step: 1320... Loss: 1.7783... Val Loss: 1.7456\nEpoch: 3/20... Step: 1330... Loss: 1.7712... Val Loss: 1.7436\nEpoch: 3/20... Step: 1340... Loss: 1.7510... Val Loss: 1.7472\nEpoch: 3/20... Step: 1350... Loss: 1.7519... Val Loss: 1.7402\nEpoch: 3/20... Step: 1360... Loss: 1.7512... Val Loss: 1.7370\nEpoch: 3/20... Step: 1370... Loss: 1.7448... Val Loss: 1.7346\nEpoch: 3/20... Step: 1380... Loss: 1.7403... Val Loss: 1.7324\nEpoch: 3/20... Step: 1390... Loss: 1.7440... Val Loss: 1.7341\nEpoch: 3/20... Step: 1400... Loss: 1.7452... Val Loss: 1.7270\nEpoch: 4/20... Step: 1410... Loss: 1.7390... Val Loss: 1.7253\nEpoch: 4/20... Step: 1420... Loss: 1.7544... Val Loss: 1.7243\nEpoch: 4/20... Step: 1430... Loss: 1.7655... Val Loss: 1.7234\nEpoch: 4/20... Step: 1440... Loss: 1.7367... Val Loss: 1.7221\nEpoch: 4/20... Step: 1450... Loss: 1.7368... Val Loss: 1.7179\nEpoch: 4/20... Step: 1460... Loss: 1.7505... Val Loss: 1.7204\nEpoch: 4/20... Step: 1470... Loss: 1.7322... Val Loss: 1.7134\nEpoch: 4/20... Step: 1480... Loss: 1.7136... Val Loss: 1.7130\nEpoch: 4/20... Step: 1490... Loss: 1.7474... Val Loss: 1.7104\nEpoch: 4/20... Step: 1500... Loss: 1.7179... Val Loss: 1.7079\nEpoch: 4/20... Step: 1510... Loss: 1.7014... Val Loss: 1.7036\nEpoch: 4/20... Step: 1520... Loss: 1.7422... Val Loss: 1.7041\nEpoch: 4/20... Step: 1530... Loss: 1.6967... Val Loss: 1.7017\nEpoch: 4/20... Step: 1540... Loss: 1.7177... Val Loss: 1.6998\nEpoch: 4/20... Step: 1550... Loss: 1.6816... Val Loss: 1.7020\nEpoch: 4/20... Step: 1560... Loss: 1.7296... Val Loss: 1.6958\nEpoch: 4/20... Step: 1570... Loss: 1.7183... Val Loss: 1.6942\nEpoch: 4/20... Step: 1580... Loss: 1.6958... Val Loss: 1.6897\nEpoch: 4/20... Step: 1590... Loss: 1.7078... Val Loss: 1.6870\nEpoch: 4/20... Step: 1600... Loss: 1.7206... Val Loss: 1.6818\nEpoch: 4/20... Step: 1610... Loss: 1.6942... Val Loss: 1.6803\nEpoch: 4/20... Step: 1620... Loss: 1.6679... Val Loss: 1.6806\nEpoch: 4/20... Step: 1630... Loss: 1.7046... Val Loss: 1.6826\nEpoch: 4/20... Step: 1640... Loss: 1.6776... Val Loss: 1.6797\nEpoch: 4/20... Step: 1650... Loss: 1.6507... Val Loss: 1.6761\nEpoch: 4/20... Step: 1660... Loss: 1.6765... Val Loss: 1.6712\nEpoch: 4/20... Step: 1670... Loss: 1.6668... Val Loss: 1.6697\nEpoch: 4/20... Step: 1680... Loss: 1.6984... Val Loss: 1.6665\nEpoch: 4/20... Step: 1690... Loss: 1.6834... Val Loss: 1.6631\nEpoch: 4/20... Step: 1700... Loss: 1.6775... Val Loss: 1.6596\nEpoch: 4/20... Step: 1710... Loss: 1.6610... Val Loss: 1.6597\nEpoch: 4/20... Step: 1720... Loss: 1.6671... Val Loss: 1.6569\nEpoch: 4/20... Step: 1730... Loss: 1.6994... Val Loss: 1.6602\nEpoch: 4/20... Step: 1740... Loss: 1.6599... Val Loss: 1.6576\nEpoch: 4/20... Step: 1750... Loss: 1.6731... Val Loss: 1.6601\nEpoch: 4/20... Step: 1760... Loss: 1.6488... Val Loss: 1.6592\nEpoch: 4/20... Step: 1770... Loss: 1.6664... Val Loss: 1.6557\nEpoch: 4/20... Step: 1780... Loss: 1.6509... Val Loss: 1.6519\nEpoch: 4/20... Step: 1790... Loss: 1.6163... Val Loss: 1.6545\nEpoch: 4/20... Step: 1800... Loss: 1.6263... Val Loss: 1.6555\nEpoch: 4/20... Step: 1810... Loss: 1.6516... Val Loss: 1.6541\nEpoch: 4/20... Step: 1820... Loss: 1.6611... Val Loss: 1.6483\nEpoch: 4/20... Step: 1830... Loss: 1.6613... Val Loss: 1.6485\nEpoch: 4/20... Step: 1840... Loss: 1.6685... Val Loss: 1.6512\nEpoch: 4/20... Step: 1850... Loss: 1.6346... Val Loss: 1.6482\nEpoch: 4/20... Step: 1860... Loss: 1.6560... Val Loss: 1.6498\nEpoch: 4/20... Step: 1870... Loss: 1.6456... Val Loss: 1.6443\nEpoch: 5/20... Step: 1880... Loss: 1.6638... Val Loss: 1.6431\nEpoch: 5/20... Step: 1890... Loss: 1.6242... Val Loss: 1.6432\nEpoch: 5/20... Step: 1900... Loss: 1.6523... Val Loss: 1.6410\nEpoch: 5/20... Step: 1910... Loss: 1.6588... Val Loss: 1.6426\nEpoch: 5/20... Step: 1920... Loss: 1.6477... Val Loss: 1.6385\nEpoch: 5/20... Step: 1930... Loss: 1.6576... Val Loss: 1.6404\nEpoch: 5/20... Step: 1940... Loss: 1.6651... Val Loss: 1.6373\nEpoch: 5/20... Step: 1950... Loss: 1.6376... Val Loss: 1.6358\nEpoch: 5/20... Step: 1960... Loss: 1.6554... Val Loss: 1.6326\nEpoch: 5/20... Step: 1970... Loss: 1.6344... Val Loss: 1.6316\nEpoch: 5/20... Step: 1980... Loss: 1.6128... Val Loss: 1.6313\nEpoch: 5/20... Step: 1990... Loss: 1.6403... Val Loss: 1.6304\nEpoch: 5/20... Step: 2000... Loss: 1.6347... Val Loss: 1.6268\nEpoch: 5/20... Step: 2010... Loss: 1.6425... Val Loss: 1.6304\nEpoch: 5/20... Step: 2020... Loss: 1.6043... Val Loss: 1.6271\nEpoch: 5/20... Step: 2030... Loss: 1.6071... Val Loss: 1.6265\nEpoch: 5/20... Step: 2040... Loss: 1.6221... Val Loss: 1.6233\nEpoch: 5/20... Step: 2050... Loss: 1.6267... Val Loss: 1.6226\nEpoch: 5/20... Step: 2060... Loss: 1.5865... Val Loss: 1.6207\nEpoch: 5/20... Step: 2070... Loss: 1.6309... Val Loss: 1.6162\nEpoch: 5/20... Step: 2080... Loss: 1.6244... Val Loss: 1.6147\nEpoch: 5/20... Step: 2090... Loss: 1.6037... Val Loss: 1.6149\nEpoch: 5/20... Step: 2100... Loss: 1.5831... Val Loss: 1.6147\nEpoch: 5/20... Step: 2110... Loss: 1.5990... Val Loss: 1.6156\nEpoch: 5/20... Step: 2120... Loss: 1.5953... Val Loss: 1.6106\nEpoch: 5/20... Step: 2130... Loss: 1.6081... Val Loss: 1.6093\nEpoch: 5/20... Step: 2140... Loss: 1.6241... Val Loss: 1.6079\nEpoch: 5/20... Step: 2150... Loss: 1.6149... Val Loss: 1.6071\nEpoch: 5/20... Step: 2160... Loss: 1.5993... Val Loss: 1.6055\nEpoch: 5/20... Step: 2170... Loss: 1.6320... Val Loss: 1.6019\nEpoch: 5/20... Step: 2180... Loss: 1.5832... Val Loss: 1.5998\nEpoch: 5/20... Step: 2190... Loss: 1.6042... Val Loss: 1.5976\nEpoch: 5/20... Step: 2200... Loss: 1.5869... Val Loss: 1.6015\nEpoch: 5/20... Step: 2210... Loss: 1.6085... Val Loss: 1.6009\nEpoch: 5/20... Step: 2220... Loss: 1.5968... Val Loss: 1.6053\nEpoch: 5/20... Step: 2230... Loss: 1.5707... Val Loss: 1.6019\nEpoch: 5/20... Step: 2240... Loss: 1.5851... Val Loss: 1.5981\nEpoch: 5/20... Step: 2250... Loss: 1.6045... Val Loss: 1.5965\nEpoch: 5/20... Step: 2260... Loss: 1.5741... Val Loss: 1.5999\nEpoch: 5/20... Step: 2270... Loss: 1.5834... Val Loss: 1.6018\nEpoch: 5/20... Step: 2280... Loss: 1.5725... Val Loss: 1.5996\nEpoch: 5/20... Step: 2290... Loss: 1.6187... Val Loss: 1.5952\nEpoch: 5/20... Step: 2300... Loss: 1.6186... Val Loss: 1.5963\nEpoch: 5/20... Step: 2310... Loss: 1.5849... Val Loss: 1.5969\nEpoch: 5/20... Step: 2320... Loss: 1.5867... Val Loss: 1.5980\nEpoch: 5/20... Step: 2330... Loss: 1.6217... Val Loss: 1.5956\nEpoch: 5/20... Step: 2340... Loss: 1.5763... Val Loss: 1.5943\nEpoch: 6/20... Step: 2350... Loss: 1.5793... Val Loss: 1.5912\nEpoch: 6/20... Step: 2360... Loss: 1.5719... Val Loss: 1.5937\nEpoch: 6/20... Step: 2370... Loss: 1.5448... Val Loss: 1.5923\nEpoch: 6/20... Step: 2380... Loss: 1.5942... Val Loss: 1.5897\nEpoch: 6/20... Step: 2390... Loss: 1.5854... Val Loss: 1.5881\nEpoch: 6/20... Step: 2400... Loss: 1.5673... Val Loss: 1.5907\nEpoch: 6/20... Step: 2410... Loss: 1.5815... Val Loss: 1.5875\nEpoch: 6/20... Step: 2420... Loss: 1.5769... Val Loss: 1.5868\nEpoch: 6/20... Step: 2430... Loss: 1.5789... Val Loss: 1.5845\nEpoch: 6/20... Step: 2440... Loss: 1.5687... Val Loss: 1.5835\nEpoch: 6/20... Step: 2450... Loss: 1.5553... Val Loss: 1.5867\nEpoch: 6/20... Step: 2460... Loss: 1.5597... Val Loss: 1.5830\nEpoch: 6/20... Step: 2470... Loss: 1.5479... Val Loss: 1.5804\nEpoch: 6/20... Step: 2480... Loss: 1.5739... Val Loss: 1.5805\nEpoch: 6/20... Step: 2490... Loss: 1.5586... Val Loss: 1.5810\nEpoch: 6/20... Step: 2500... Loss: 1.5656... Val Loss: 1.5812\nEpoch: 6/20... Step: 2510... Loss: 1.5997... Val Loss: 1.5786\nEpoch: 6/20... Step: 2520... Loss: 1.5445... Val Loss: 1.5758\nEpoch: 6/20... Step: 2530... Loss: 1.5669... Val Loss: 1.5761\nEpoch: 6/20... Step: 2540... Loss: 1.5986... Val Loss: 1.5736\nEpoch: 6/20... Step: 2550... Loss: 1.5584... Val Loss: 1.5719\nEpoch: 6/20... Step: 2560... Loss: 1.5599... Val Loss: 1.5739\nEpoch: 6/20... Step: 2570... Loss: 1.5434... Val Loss: 1.5714\nEpoch: 6/20... Step: 2580... Loss: 1.5298... Val Loss: 1.5729\nEpoch: 6/20... Step: 2590... Loss: 1.5459... Val Loss: 1.5684\nEpoch: 6/20... Step: 2600... Loss: 1.5541... Val Loss: 1.5668\nEpoch: 6/20... Step: 2610... Loss: 1.5243... Val Loss: 1.5657\nEpoch: 6/20... Step: 2620... Loss: 1.5471... Val Loss: 1.5676\nEpoch: 6/20... Step: 2630... Loss: 1.5332... Val Loss: 1.5640\nEpoch: 6/20... Step: 2640... Loss: 1.5683... Val Loss: 1.5617\nEpoch: 6/20... Step: 2650... Loss: 1.5353... Val Loss: 1.5606\nEpoch: 6/20... Step: 2660... Loss: 1.5385... Val Loss: 1.5597\nEpoch: 6/20... Step: 2670... Loss: 1.5286... Val Loss: 1.5629\nEpoch: 6/20... Step: 2680... Loss: 1.5534... Val Loss: 1.5604\nEpoch: 6/20... Step: 2690... Loss: 1.5485... Val Loss: 1.5622\nEpoch: 6/20... Step: 2700... Loss: 1.5501... Val Loss: 1.5629\nEpoch: 6/20... Step: 2710... Loss: 1.5394... Val Loss: 1.5610\nEpoch: 6/20... Step: 2720... Loss: 1.5598... Val Loss: 1.5579\nEpoch: 6/20... Step: 2730... Loss: 1.5433... Val Loss: 1.5597\nEpoch: 6/20... Step: 2740... Loss: 1.5396... Val Loss: 1.5604\nEpoch: 6/20... Step: 2750... Loss: 1.5533... Val Loss: 1.5619\nEpoch: 6/20... Step: 2760... Loss: 1.5897... Val Loss: 1.5562\nEpoch: 6/20... Step: 2770... Loss: 1.5235... Val Loss: 1.5574\nEpoch: 6/20... Step: 2780... Loss: 1.5089... Val Loss: 1.5569\nEpoch: 6/20... Step: 2790... Loss: 1.5203... Val Loss: 1.5595\nEpoch: 6/20... Step: 2800... Loss: 1.5257... Val Loss: 1.5592\nEpoch: 6/20... Step: 2810... Loss: 1.5355... Val Loss: 1.5570\nEpoch: 7/20... Step: 2820... Loss: 1.5202... Val Loss: 1.5551\nEpoch: 7/20... Step: 2830... Loss: 1.5516... Val Loss: 1.5569\nEpoch: 7/20... Step: 2840... Loss: 1.5606... Val Loss: 1.5539\nEpoch: 7/20... Step: 2850... Loss: 1.5202... Val Loss: 1.5535\nEpoch: 7/20... Step: 2860... Loss: 1.5362... Val Loss: 1.5524\nEpoch: 7/20... Step: 2870... Loss: 1.4940... Val Loss: 1.5554\nEpoch: 7/20... Step: 2880... Loss: 1.5390... Val Loss: 1.5517\nEpoch: 7/20... Step: 2890... Loss: 1.5324... Val Loss: 1.5529\nEpoch: 7/20... Step: 2900... Loss: 1.5019... Val Loss: 1.5498\nEpoch: 7/20... Step: 2910... Loss: 1.5145... Val Loss: 1.5486\nEpoch: 7/20... Step: 2920... Loss: 1.5131... Val Loss: 1.5539\nEpoch: 7/20... Step: 2930... Loss: 1.5083... Val Loss: 1.5495\nEpoch: 7/20... Step: 2940... Loss: 1.5191... Val Loss: 1.5495\nEpoch: 7/20... Step: 2950... Loss: 1.5250... Val Loss: 1.5459\nEpoch: 7/20... Step: 2960... Loss: 1.5360... Val Loss: 1.5478\nEpoch: 7/20... Step: 2970... Loss: 1.5475... Val Loss: 1.5476\nEpoch: 7/20... Step: 2980... Loss: 1.5452... Val Loss: 1.5457\nEpoch: 7/20... Step: 2990... Loss: 1.5280... Val Loss: 1.5449\nEpoch: 7/20... Step: 3000... Loss: 1.5189... Val Loss: 1.5437\nEpoch: 7/20... Step: 3010... Loss: 1.5090... Val Loss: 1.5428\nEpoch: 7/20... Step: 3020... Loss: 1.5282... Val Loss: 1.5386\nEpoch: 7/20... Step: 3030... Loss: 1.5006... Val Loss: 1.5420\nEpoch: 7/20... Step: 3040... Loss: 1.4854... Val Loss: 1.5448\nEpoch: 7/20... Step: 3050... Loss: 1.4991... Val Loss: 1.5448\nEpoch: 7/20... Step: 3060... Loss: 1.5058... Val Loss: 1.5431\nEpoch: 7/20... Step: 3070... Loss: 1.5148... Val Loss: 1.5355\nEpoch: 7/20... Step: 3080... Loss: 1.4969... Val Loss: 1.5363\nEpoch: 7/20... Step: 3090... Loss: 1.5025... Val Loss: 1.5336\nEpoch: 7/20... Step: 3100... Loss: 1.5112... Val Loss: 1.5365\nEpoch: 7/20... Step: 3110... Loss: 1.5194... Val Loss: 1.5353\nEpoch: 7/20... Step: 3120... Loss: 1.5221... Val Loss: 1.5321\nEpoch: 7/20... Step: 3130... Loss: 1.5156... Val Loss: 1.5324\nEpoch: 7/20... Step: 3140... Loss: 1.4952... Val Loss: 1.5359\nEpoch: 7/20... Step: 3150... Loss: 1.4958... Val Loss: 1.5343\nEpoch: 7/20... Step: 3160... Loss: 1.5188... Val Loss: 1.5387\nEpoch: 7/20... Step: 3170... Loss: 1.5082... Val Loss: 1.5367\nEpoch: 7/20... Step: 3180... Loss: 1.5238... Val Loss: 1.5339\nEpoch: 7/20... Step: 3190... Loss: 1.5032... Val Loss: 1.5316\nEpoch: 7/20... Step: 3200... Loss: 1.5092... Val Loss: 1.5329\nEpoch: 7/20... Step: 3210... Loss: 1.4735... Val Loss: 1.5340\nEpoch: 7/20... Step: 3220... Loss: 1.5147... Val Loss: 1.5377\nEpoch: 7/20... Step: 3230... Loss: 1.5047... Val Loss: 1.5335\nEpoch: 7/20... Step: 3240... Loss: 1.5021... Val Loss: 1.5324\nEpoch: 7/20... Step: 3250... Loss: 1.4978... Val Loss: 1.5328\nEpoch: 7/20... Step: 3260... Loss: 1.5158... Val Loss: 1.5348\nEpoch: 7/20... Step: 3270... Loss: 1.4937... Val Loss: 1.5327\nEpoch: 7/20... Step: 3280... Loss: 1.5080... Val Loss: 1.5329\nEpoch: 8/20... Step: 3290... Loss: 1.4807... Val Loss: 1.5318\nEpoch: 8/20... Step: 3300... Loss: 1.5032... Val Loss: 1.5320\nEpoch: 8/20... Step: 3310... Loss: 1.5155... Val Loss: 1.5311\nEpoch: 8/20... Step: 3320... Loss: 1.4914... Val Loss: 1.5286\nEpoch: 8/20... Step: 3330... Loss: 1.4998... Val Loss: 1.5262\nEpoch: 8/20... Step: 3340... Loss: 1.4991... Val Loss: 1.5287\nEpoch: 8/20... Step: 3350... Loss: 1.5191... Val Loss: 1.5274\nEpoch: 8/20... Step: 3360... Loss: 1.4638... Val Loss: 1.5267\nEpoch: 8/20... Step: 3370... Loss: 1.4983... Val Loss: 1.5266\nEpoch: 8/20... Step: 3380... Loss: 1.4858... Val Loss: 1.5245\nEpoch: 8/20... Step: 3390... Loss: 1.4599... Val Loss: 1.5296\nEpoch: 8/20... Step: 3400... Loss: 1.4862... Val Loss: 1.5268\nEpoch: 8/20... Step: 3410... Loss: 1.4736... Val Loss: 1.5272\nEpoch: 8/20... Step: 3420... Loss: 1.4790... Val Loss: 1.5239\nEpoch: 8/20... Step: 3430... Loss: 1.4989... Val Loss: 1.5262\nEpoch: 8/20... Step: 3440... Loss: 1.5059... Val Loss: 1.5247\nEpoch: 8/20... Step: 3450... Loss: 1.5055... Val Loss: 1.5233\nEpoch: 8/20... Step: 3460... Loss: 1.4528... Val Loss: 1.5198\nEpoch: 8/20... Step: 3470... Loss: 1.4763... Val Loss: 1.5216\nEpoch: 8/20... Step: 3480... Loss: 1.5047... Val Loss: 1.5240\nEpoch: 8/20... Step: 3490... Loss: 1.4597... Val Loss: 1.5174\nEpoch: 8/20... Step: 3500... Loss: 1.4729... Val Loss: 1.5178\nEpoch: 8/20... Step: 3510... Loss: 1.4721... Val Loss: 1.5189\nEpoch: 8/20... Step: 3520... Loss: 1.4820... Val Loss: 1.5239\nEpoch: 8/20... Step: 3530... Loss: 1.5008... Val Loss: 1.5211\nEpoch: 8/20... Step: 3540... Loss: 1.4760... Val Loss: 1.5182\nEpoch: 8/20... Step: 3550... Loss: 1.4997... Val Loss: 1.5137\nEpoch: 8/20... Step: 3560... Loss: 1.4854... Val Loss: 1.5153\nEpoch: 8/20... Step: 3570... Loss: 1.4650... Val Loss: 1.5143\nEpoch: 8/20... Step: 3580... Loss: 1.4615... Val Loss: 1.5147\nEpoch: 8/20... Step: 3590... Loss: 1.4715... Val Loss: 1.5113\nEpoch: 8/20... Step: 3600... Loss: 1.4832... Val Loss: 1.5113\nEpoch: 8/20... Step: 3610... Loss: 1.4499... Val Loss: 1.5137\nEpoch: 8/20... Step: 3620... Loss: 1.4766... Val Loss: 1.5135\nEpoch: 8/20... Step: 3630... Loss: 1.4779... Val Loss: 1.5181\nEpoch: 8/20... Step: 3640... Loss: 1.4959... Val Loss: 1.5144\nEpoch: 8/20... Step: 3650... Loss: 1.4734... Val Loss: 1.5154\nEpoch: 8/20... Step: 3660... Loss: 1.4990... Val Loss: 1.5117\nEpoch: 8/20... Step: 3670... Loss: 1.4427... Val Loss: 1.5148\nEpoch: 8/20... Step: 3680... Loss: 1.4504... Val Loss: 1.5127\nEpoch: 8/20... Step: 3690... Loss: 1.4765... Val Loss: 1.5146\nEpoch: 8/20... Step: 3700... Loss: 1.4661... Val Loss: 1.5107\nEpoch: 8/20... Step: 3710... Loss: 1.4492... Val Loss: 1.5116\nEpoch: 8/20... Step: 3720... Loss: 1.4711... Val Loss: 1.5131\nEpoch: 8/20... Step: 3730... Loss: 1.4480... Val Loss: 1.5139\nEpoch: 8/20... Step: 3740... Loss: 1.4780... Val Loss: 1.5135\nEpoch: 8/20... Step: 3750... Loss: 1.4749... Val Loss: 1.5131\nEpoch: 9/20... Step: 3760... Loss: 1.4877... Val Loss: 1.5116\nEpoch: 9/20... Step: 3770... Loss: 1.4588... Val Loss: 1.5121\nEpoch: 9/20... Step: 3780... Loss: 1.4687... Val Loss: 1.5101\nEpoch: 9/20... Step: 3790... Loss: 1.4634... Val Loss: 1.5092\nEpoch: 9/20... Step: 3800... Loss: 1.4576... Val Loss: 1.5086\nEpoch: 9/20... Step: 3810... Loss: 1.4552... Val Loss: 1.5102\nEpoch: 9/20... Step: 3820... Loss: 1.4418... Val Loss: 1.5072\nEpoch: 9/20... Step: 3830... Loss: 1.4616... Val Loss: 1.5104\nEpoch: 9/20... Step: 3840... Loss: 1.4703... Val Loss: 1.5104\nEpoch: 9/20... Step: 3850... Loss: 1.4612... Val Loss: 1.5051\nEpoch: 9/20... Step: 3860... Loss: 1.4399... Val Loss: 1.5115\nEpoch: 9/20... Step: 3870... Loss: 1.4705... Val Loss: 1.5060\nEpoch: 9/20... Step: 3880... Loss: 1.4630... Val Loss: 1.5068\nEpoch: 9/20... Step: 3890... Loss: 1.4493... Val Loss: 1.5063\nEpoch: 9/20... Step: 3900... Loss: 1.4434... Val Loss: 1.5072\nEpoch: 9/20... Step: 3910... Loss: 1.4671... Val Loss: 1.5057\nEpoch: 9/20... Step: 3920... Loss: 1.4482... Val Loss: 1.5055\nEpoch: 9/20... Step: 3930... Loss: 1.4630... Val Loss: 1.5055\nEpoch: 9/20... Step: 3940... Loss: 1.4604... Val Loss: 1.5033\nEpoch: 9/20... Step: 3950... Loss: 1.4647... Val Loss: 1.5051\nEpoch: 9/20... Step: 3960... Loss: 1.4783... Val Loss: 1.4997\nEpoch: 9/20... Step: 3970... Loss: 1.4918... Val Loss: 1.5025\nEpoch: 9/20... Step: 3980... Loss: 1.4782... Val Loss: 1.5039\nEpoch: 9/20... Step: 3990... Loss: 1.4839... Val Loss: 1.5060\nEpoch: 9/20... Step: 4000... Loss: 1.4592... Val Loss: 1.5024\nEpoch: 9/20... Step: 4010... Loss: 1.4274... Val Loss: 1.5020\nEpoch: 9/20... Step: 4020... Loss: 1.4458... Val Loss: 1.4973\nEpoch: 9/20... Step: 4030... Loss: 1.4596... Val Loss: 1.4972\nEpoch: 9/20... Step: 4040... Loss: 1.4642... Val Loss: 1.4991\nEpoch: 9/20... Step: 4050... Loss: 1.4627... Val Loss: 1.5003\nEpoch: 9/20... Step: 4060... Loss: 1.4381... Val Loss: 1.4960\nEpoch: 9/20... Step: 4070... Loss: 1.4550... Val Loss: 1.4949\nEpoch: 9/20... Step: 4080... Loss: 1.4723... Val Loss: 1.4979\nEpoch: 9/20... Step: 4090... Loss: 1.4570... Val Loss: 1.4956\nEpoch: 9/20... Step: 4100... Loss: 1.4418... Val Loss: 1.4977\nEpoch: 9/20... Step: 4110... Loss: 1.4266... Val Loss: 1.4989\nEpoch: 9/20... Step: 4120... Loss: 1.4693... Val Loss: 1.4983\nEpoch: 9/20... Step: 4130... Loss: 1.4418... Val Loss: 1.4988\nEpoch: 9/20... Step: 4140... Loss: 1.4417... Val Loss: 1.4972\nEpoch: 9/20... Step: 4150... Loss: 1.4475... Val Loss: 1.4987\nEpoch: 9/20... Step: 4160... Loss: 1.4614... Val Loss: 1.4995\nEpoch: 9/20... Step: 4170... Loss: 1.4631... Val Loss: 1.4967\nEpoch: 9/20... Step: 4180... Loss: 1.4666... Val Loss: 1.4944\nEpoch: 9/20... Step: 4190... Loss: 1.4399... Val Loss: 1.4979\nEpoch: 9/20... Step: 4200... Loss: 1.4185... Val Loss: 1.4977\nEpoch: 9/20... Step: 4210... Loss: 1.4590... Val Loss: 1.4981\nEpoch: 9/20... Step: 4220... Loss: 1.4409... Val Loss: 1.4990\nEpoch: 10/20... Step: 4230... Loss: 1.4443... Val Loss: 1.4954\nEpoch: 10/20... Step: 4240... Loss: 1.4549... Val Loss: 1.5008\nEpoch: 10/20... Step: 4250... Loss: 1.4624... Val Loss: 1.4957\nEpoch: 10/20... Step: 4260... Loss: 1.4409... Val Loss: 1.4955\nEpoch: 10/20... Step: 4270... Loss: 1.4757... Val Loss: 1.4959\nEpoch: 10/20... Step: 4280... Loss: 1.4383... Val Loss: 1.4969\nEpoch: 10/20... Step: 4290... Loss: 1.4159... Val Loss: 1.4947\nEpoch: 10/20... Step: 4300... Loss: 1.4283... Val Loss: 1.4976\nEpoch: 10/20... Step: 4310... Loss: 1.4501... Val Loss: 1.4937\nEpoch: 10/20... Step: 4320... Loss: 1.4359... Val Loss: 1.4911\nEpoch: 10/20... Step: 4330... Loss: 1.4301... Val Loss: 1.4982\nEpoch: 10/20... Step: 4340... Loss: 1.4376... Val Loss: 1.4936\nEpoch: 10/20... Step: 4350... Loss: 1.4381... Val Loss: 1.4938\nEpoch: 10/20... Step: 4360... Loss: 1.4409... Val Loss: 1.4927\nEpoch: 10/20... Step: 4370... Loss: 1.4558... Val Loss: 1.4947\nEpoch: 10/20... Step: 4380... Loss: 1.4384... Val Loss: 1.4926\nEpoch: 10/20... Step: 4390... Loss: 1.4265... Val Loss: 1.4927\nEpoch: 10/20... Step: 4400... Loss: 1.4309... Val Loss: 1.4911\nEpoch: 10/20... Step: 4410... Loss: 1.4342... Val Loss: 1.4901\nEpoch: 10/20... Step: 4420... Loss: 1.4529... Val Loss: 1.4967\nEpoch: 10/20... Step: 4430... Loss: 1.4191... Val Loss: 1.4875\nEpoch: 10/20... Step: 4440... Loss: 1.4313... Val Loss: 1.4909\nEpoch: 10/20... Step: 4450... Loss: 1.4383... Val Loss: 1.4926\nEpoch: 10/20... Step: 4460... Loss: 1.4296... Val Loss: 1.4914\nEpoch: 10/20... Step: 4470... Loss: 1.4391... Val Loss: 1.4895\nEpoch: 10/20... Step: 4480... Loss: 1.4419... Val Loss: 1.4907\nEpoch: 10/20... Step: 4490... Loss: 1.4374... Val Loss: 1.4829\nEpoch: 10/20... Step: 4500... Loss: 1.4548... Val Loss: 1.4838\nEpoch: 10/20... Step: 4510... Loss: 1.4367... Val Loss: 1.4843\nEpoch: 10/20... Step: 4520... Loss: 1.4274... Val Loss: 1.4878\nEpoch: 10/20... Step: 4530... Loss: 1.4175... Val Loss: 1.4827\nEpoch: 10/20... Step: 4540... Loss: 1.4215... Val Loss: 1.4816\nEpoch: 10/20... Step: 4550... Loss: 1.4591... Val Loss: 1.4859\nEpoch: 10/20... Step: 4560... Loss: 1.4354... Val Loss: 1.4832\nEpoch: 10/20... Step: 4570... Loss: 1.4150... Val Loss: 1.4855\nEpoch: 10/20... Step: 4580... Loss: 1.4177... Val Loss: 1.4841\nEpoch: 10/20... Step: 4590... Loss: 1.4336... Val Loss: 1.4846\nEpoch: 10/20... Step: 4600... Loss: 1.4159... Val Loss: 1.4835\nEpoch: 10/20... Step: 4610... Loss: 1.4324... Val Loss: 1.4874\nEpoch: 10/20... Step: 4620... Loss: 1.4180... Val Loss: 1.4859\nEpoch: 10/20... Step: 4630... Loss: 1.4260... Val Loss: 1.4885\nEpoch: 10/20... Step: 4640... Loss: 1.4329... Val Loss: 1.4868\nEpoch: 10/20... Step: 4650... Loss: 1.4277... Val Loss: 1.4823\nEpoch: 10/20... Step: 4660... Loss: 1.4167... Val Loss: 1.4879\nEpoch: 10/20... Step: 4670... Loss: 1.4260... Val Loss: 1.4870\nEpoch: 10/20... Step: 4680... Loss: 1.4600... Val Loss: 1.4854\nEpoch: 10/20... Step: 4690... Loss: 1.4329... Val Loss: 1.4865\nEpoch: 11/20... Step: 4700... Loss: 1.4241... Val Loss: 1.4850\nEpoch: 11/20... Step: 4710... Loss: 1.4394... Val Loss: 1.4862\nEpoch: 11/20... Step: 4720... Loss: 1.4360... Val Loss: 1.4837\nEpoch: 11/20... Step: 4730... Loss: 1.4165... Val Loss: 1.4841\nEpoch: 11/20... Step: 4740... Loss: 1.4334... Val Loss: 1.4837\nEpoch: 11/20... Step: 4750... Loss: 1.4273... Val Loss: 1.4869\nEpoch: 11/20... Step: 4760... Loss: 1.4379... Val Loss: 1.4826\nEpoch: 11/20... Step: 4770... Loss: 1.4204... Val Loss: 1.4866\nEpoch: 11/20... Step: 4780... Loss: 1.4393... Val Loss: 1.4827\nEpoch: 11/20... Step: 4790... Loss: 1.3951... Val Loss: 1.4809\nEpoch: 11/20... Step: 4800... Loss: 1.4122... Val Loss: 1.4831\nEpoch: 11/20... Step: 4810... Loss: 1.4175... Val Loss: 1.4834\nEpoch: 11/20... Step: 4820... Loss: 1.4024... Val Loss: 1.4801\nEpoch: 11/20... Step: 4830... Loss: 1.4179... Val Loss: 1.4834\nEpoch: 11/20... Step: 4840... Loss: 1.4230... Val Loss: 1.4813\nEpoch: 11/20... Step: 4850... Loss: 1.4307... Val Loss: 1.4798\nEpoch: 11/20... Step: 4860... Loss: 1.4083... Val Loss: 1.4757\nEpoch: 11/20... Step: 4870... Loss: 1.4140... Val Loss: 1.4803\nEpoch: 11/20... Step: 4880... Loss: 1.4148... Val Loss: 1.4771\nEpoch: 11/20... Step: 4890... Loss: 1.4507... Val Loss: 1.4801\nEpoch: 11/20... Step: 4900... Loss: 1.4091... Val Loss: 1.4775\nEpoch: 11/20... Step: 4910... Loss: 1.4084... Val Loss: 1.4786\nEpoch: 11/20... Step: 4920... Loss: 1.3986... Val Loss: 1.4830\nEpoch: 11/20... Step: 4930... Loss: 1.3938... Val Loss: 1.4803\nEpoch: 11/20... Step: 4940... Loss: 1.4006... Val Loss: 1.4815\nEpoch: 11/20... Step: 4950... Loss: 1.3826... Val Loss: 1.4812\nEpoch: 11/20... Step: 4960... Loss: 1.4108... Val Loss: 1.4739\nEpoch: 11/20... Step: 4970... Loss: 1.4086... Val Loss: 1.4745\nEpoch: 11/20... Step: 4980... Loss: 1.4100... Val Loss: 1.4744\nEpoch: 11/20... Step: 4990... Loss: 1.4167... Val Loss: 1.4795\nEpoch: 11/20... Step: 5000... Loss: 1.3994... Val Loss: 1.4725\nEpoch: 11/20... Step: 5010... Loss: 1.4132... Val Loss: 1.4699\nEpoch: 11/20... Step: 5020... Loss: 1.4132... Val Loss: 1.4729\nEpoch: 11/20... Step: 5030... Loss: 1.4130... Val Loss: 1.4718\nEpoch: 11/20... Step: 5040... Loss: 1.4056... Val Loss: 1.4765\nEpoch: 11/20... Step: 5050... Loss: 1.4243... Val Loss: 1.4759\nEpoch: 11/20... Step: 5060... Loss: 1.4039... Val Loss: 1.4736\nEpoch: 11/20... Step: 5070... Loss: 1.3975... Val Loss: 1.4723\nEpoch: 11/20... Step: 5080... Loss: 1.4218... Val Loss: 1.4753\nEpoch: 11/20... Step: 5090... Loss: 1.3910... Val Loss: 1.4781\nEpoch: 11/20... Step: 5100... Loss: 1.4392... Val Loss: 1.4770\nEpoch: 11/20... Step: 5110... Loss: 1.3934... Val Loss: 1.4776\nEpoch: 11/20... Step: 5120... Loss: 1.4054... Val Loss: 1.4707\nEpoch: 11/20... Step: 5130... Loss: 1.4010... Val Loss: 1.4804\nEpoch: 11/20... Step: 5140... Loss: 1.3893... Val Loss: 1.4772\nEpoch: 11/20... Step: 5150... Loss: 1.4177... Val Loss: 1.4762\nEpoch: 12/20... Step: 5160... Loss: 1.4327... Val Loss: 1.4758\nEpoch: 12/20... Step: 5170... Loss: 1.4125... Val Loss: 1.4746\nEpoch: 12/20... Step: 5180... Loss: 1.4155... Val Loss: 1.4757\nEpoch: 12/20... Step: 5190... Loss: 1.4102... Val Loss: 1.4732\nEpoch: 12/20... Step: 5200... Loss: 1.3985... Val Loss: 1.4751\nEpoch: 12/20... Step: 5210... Loss: 1.4141... Val Loss: 1.4753\nEpoch: 12/20... Step: 5220... Loss: 1.3731... Val Loss: 1.4750\nEpoch: 12/20... Step: 5230... Loss: 1.4137... Val Loss: 1.4712\nEpoch: 12/20... Step: 5240... Loss: 1.4232... Val Loss: 1.4764\nEpoch: 12/20... Step: 5250... Loss: 1.4270... Val Loss: 1.4738\nEpoch: 12/20... Step: 5260... Loss: 1.4005... Val Loss: 1.4713\nEpoch: 12/20... Step: 5270... Loss: 1.4039... Val Loss: 1.4744\nEpoch: 12/20... Step: 5280... Loss: 1.4214... Val Loss: 1.4698\nEpoch: 12/20... Step: 5290... Loss: 1.3868... Val Loss: 1.4735\nEpoch: 12/20... Step: 5300... Loss: 1.3907... Val Loss: 1.4706\nEpoch: 12/20... Step: 5310... Loss: 1.4193... Val Loss: 1.4724\nEpoch: 12/20... Step: 5320... Loss: 1.4087... Val Loss: 1.4687\nEpoch: 12/20... Step: 5330... Loss: 1.3913... Val Loss: 1.4700\nEpoch: 12/20... Step: 5340... Loss: 1.3653... Val Loss: 1.4690\nEpoch: 12/20... Step: 5350... Loss: 1.4182... Val Loss: 1.4700\nEpoch: 12/20... Step: 5360... Loss: 1.4219... Val Loss: 1.4770\nEpoch: 12/20... Step: 5370... Loss: 1.4015... Val Loss: 1.4709\nEpoch: 12/20... Step: 5380... Loss: 1.4058... Val Loss: 1.4695\nEpoch: 12/20... Step: 5390... Loss: 1.3869... Val Loss: 1.4733\nEpoch: 12/20... Step: 5400... Loss: 1.3735... Val Loss: 1.4728\nEpoch: 12/20... Step: 5410... Loss: 1.3721... Val Loss: 1.4728\nEpoch: 12/20... Step: 5420... Loss: 1.4015... Val Loss: 1.4744\nEpoch: 12/20... Step: 5430... Loss: 1.4113... Val Loss: 1.4649\nEpoch: 12/20... Step: 5440... Loss: 1.3762... Val Loss: 1.4651\nEpoch: 12/20... Step: 5450... Loss: 1.3867... Val Loss: 1.4659\nEpoch: 12/20... Step: 5460... Loss: 1.4145... Val Loss: 1.4684\nEpoch: 12/20... Step: 5470... Loss: 1.3910... Val Loss: 1.4671\nEpoch: 12/20... Step: 5480... Loss: 1.4067... Val Loss: 1.4612\nEpoch: 12/20... Step: 5490... Loss: 1.3925... Val Loss: 1.4666\nEpoch: 12/20... Step: 5500... Loss: 1.3552... Val Loss: 1.4637\nEpoch: 12/20... Step: 5510... Loss: 1.4285... Val Loss: 1.4680\nEpoch: 12/20... Step: 5520... Loss: 1.3861... Val Loss: 1.4679\nEpoch: 12/20... Step: 5530... Loss: 1.3963... Val Loss: 1.4669\nEpoch: 12/20... Step: 5540... Loss: 1.3986... Val Loss: 1.4647\nEpoch: 12/20... Step: 5550... Loss: 1.3866... Val Loss: 1.4685\nEpoch: 12/20... Step: 5560... Loss: 1.4148... Val Loss: 1.4712\nEpoch: 12/20... Step: 5570... Loss: 1.3937... Val Loss: 1.4692\nEpoch: 12/20... Step: 5580... Loss: 1.4097... Val Loss: 1.4687\nEpoch: 12/20... Step: 5590... Loss: 1.3732... Val Loss: 1.4679\nEpoch: 12/20... Step: 5600... Loss: 1.3771... Val Loss: 1.4684\nEpoch: 12/20... Step: 5610... Loss: 1.3881... Val Loss: 1.4683\nEpoch: 12/20... Step: 5620... Loss: 1.3907... Val Loss: 1.4664\nEpoch: 13/20... Step: 5630... Loss: 1.4149... Val Loss: 1.4718\nEpoch: 13/20... Step: 5640... Loss: 1.3755... Val Loss: 1.4681\nEpoch: 13/20... Step: 5650... Loss: 1.3952... Val Loss: 1.4684\nEpoch: 13/20... Step: 5660... Loss: 1.3775... Val Loss: 1.4677\nEpoch: 13/20... Step: 5670... Loss: 1.3897... Val Loss: 1.4649\nEpoch: 13/20... Step: 5680... Loss: 1.4282... Val Loss: 1.4682\nEpoch: 13/20... Step: 5690... Loss: 1.3998... Val Loss: 1.4680\nEpoch: 13/20... Step: 5700... Loss: 1.3706... Val Loss: 1.4665\nEpoch: 13/20... Step: 5710... Loss: 1.3825... Val Loss: 1.4685\nEpoch: 13/20... Step: 5720... Loss: 1.3822... Val Loss: 1.4653\nEpoch: 13/20... Step: 5730... Loss: 1.3999... Val Loss: 1.4635\nEpoch: 13/20... Step: 5740... Loss: 1.3821... Val Loss: 1.4657\nEpoch: 13/20... Step: 5750... Loss: 1.4014... Val Loss: 1.4636\nEpoch: 13/20... Step: 5760... Loss: 1.3835... Val Loss: 1.4660\nEpoch: 13/20... Step: 5770... Loss: 1.4036... Val Loss: 1.4640\nEpoch: 13/20... Step: 5780... Loss: 1.3873... Val Loss: 1.4639\nEpoch: 13/20... Step: 5790... Loss: 1.4229... Val Loss: 1.4628\nEpoch: 13/20... Step: 5800... Loss: 1.3637... Val Loss: 1.4622\nEpoch: 13/20... Step: 5810... Loss: 1.3557... Val Loss: 1.4612\nEpoch: 13/20... Step: 5820... Loss: 1.3660... Val Loss: 1.4620\nEpoch: 13/20... Step: 5830... Loss: 1.4017... Val Loss: 1.4656\nEpoch: 13/20... Step: 5840... Loss: 1.3874... Val Loss: 1.4636\nEpoch: 13/20... Step: 5850... Loss: 1.3700... Val Loss: 1.4639\nEpoch: 13/20... Step: 5860... Loss: 1.3940... Val Loss: 1.4622\nEpoch: 13/20... Step: 5870... Loss: 1.3851... Val Loss: 1.4622\nEpoch: 13/20... Step: 5880... Loss: 1.3894... Val Loss: 1.4623\nEpoch: 13/20... Step: 5890... Loss: 1.3789... Val Loss: 1.4657\nEpoch: 13/20... Step: 5900... Loss: 1.3899... Val Loss: 1.4587\nEpoch: 13/20... Step: 5910... Loss: 1.3991... Val Loss: 1.4602\nEpoch: 13/20... Step: 5920... Loss: 1.3784... Val Loss: 1.4582\nEpoch: 13/20... Step: 5930... Loss: 1.3620... Val Loss: 1.4625\nEpoch: 13/20... Step: 5940... Loss: 1.3971... Val Loss: 1.4597\nEpoch: 13/20... Step: 5950... Loss: 1.3759... Val Loss: 1.4582\nEpoch: 13/20... Step: 5960... Loss: 1.3773... Val Loss: 1.4595\nEpoch: 13/20... Step: 5970... Loss: 1.3753... Val Loss: 1.4601\nEpoch: 13/20... Step: 5980... Loss: 1.3802... Val Loss: 1.4602\nEpoch: 13/20... Step: 5990... Loss: 1.3930... Val Loss: 1.4630\nEpoch: 13/20... Step: 6000... Loss: 1.3864... Val Loss: 1.4586\nEpoch: 13/20... Step: 6010... Loss: 1.3819... Val Loss: 1.4579\nEpoch: 13/20... Step: 6020... Loss: 1.3823... Val Loss: 1.4693\nEpoch: 13/20... Step: 6030... Loss: 1.3838... Val Loss: 1.4627\nEpoch: 13/20... Step: 6040... Loss: 1.3694... Val Loss: 1.4600\nEpoch: 13/20... Step: 6050... Loss: 1.3655... Val Loss: 1.4654\nEpoch: 13/20... Step: 6060... Loss: 1.3737... Val Loss: 1.4603\nEpoch: 13/20... Step: 6070... Loss: 1.3713... Val Loss: 1.4587\nEpoch: 13/20... Step: 6080... Loss: 1.3678... Val Loss: 1.4646\nEpoch: 13/20... Step: 6090... Loss: 1.3683... Val Loss: 1.4579\nEpoch: 14/20... Step: 6100... Loss: 1.3746... Val Loss: 1.4614\nEpoch: 14/20... Step: 6110... Loss: 1.3745... Val Loss: 1.4611\nEpoch: 14/20... Step: 6120... Loss: 1.3932... Val Loss: 1.4622\nEpoch: 14/20... Step: 6130... Loss: 1.3816... Val Loss: 1.4606\nEpoch: 14/20... Step: 6140... Loss: 1.3661... Val Loss: 1.4580\nEpoch: 14/20... Step: 6150... Loss: 1.3981... Val Loss: 1.4621\nEpoch: 14/20... Step: 6160... Loss: 1.3944... Val Loss: 1.4611\nEpoch: 14/20... Step: 6170... Loss: 1.3663... Val Loss: 1.4616\nEpoch: 14/20... Step: 6180... Loss: 1.3842... Val Loss: 1.4645\nEpoch: 14/20... Step: 6190... Loss: 1.3640... Val Loss: 1.4600\nEpoch: 14/20... Step: 6200... Loss: 1.3773... Val Loss: 1.4581\nEpoch: 14/20... Step: 6210... Loss: 1.3959... Val Loss: 1.4642\nEpoch: 14/20... Step: 6220... Loss: 1.3605... Val Loss: 1.4564\nEpoch: 14/20... Step: 6230... Loss: 1.3868... Val Loss: 1.4580\nEpoch: 14/20... Step: 6240... Loss: 1.3573... Val Loss: 1.4594\nEpoch: 14/20... Step: 6250... Loss: 1.4097... Val Loss: 1.4595\nEpoch: 14/20... Step: 6260... Loss: 1.3985... Val Loss: 1.4555\nEpoch: 14/20... Step: 6270... Loss: 1.3486... Val Loss: 1.4570\nEpoch: 14/20... Step: 6280... Loss: 1.3743... Val Loss: 1.4554\nEpoch: 14/20... Step: 6290... Loss: 1.3850... Val Loss: 1.4542\nEpoch: 14/20... Step: 6300... Loss: 1.3733... Val Loss: 1.4646\nEpoch: 14/20... Step: 6310... Loss: 1.3530... Val Loss: 1.4607\nEpoch: 14/20... Step: 6320... Loss: 1.3908... Val Loss: 1.4625\nEpoch: 14/20... Step: 6330... Loss: 1.3677... Val Loss: 1.4562\nEpoch: 14/20... Step: 6340... Loss: 1.3283... Val Loss: 1.4562\nEpoch: 14/20... Step: 6350... Loss: 1.3673... Val Loss: 1.4544\nEpoch: 14/20... Step: 6360... Loss: 1.3607... Val Loss: 1.4605\nEpoch: 14/20... Step: 6370... Loss: 1.3703... Val Loss: 1.4512\nEpoch: 14/20... Step: 6380... Loss: 1.3718... Val Loss: 1.4533\nEpoch: 14/20... Step: 6390... Loss: 1.3696... Val Loss: 1.4516\nEpoch: 14/20... Step: 6400... Loss: 1.3719... Val Loss: 1.4563\nEpoch: 14/20... Step: 6410... Loss: 1.3782... Val Loss: 1.4538\nEpoch: 14/20... Step: 6420... Loss: 1.3871... Val Loss: 1.4527\nEpoch: 14/20... Step: 6430... Loss: 1.3660... Val Loss: 1.4548\nEpoch: 14/20... Step: 6440... Loss: 1.3676... Val Loss: 1.4499\nEpoch: 14/20... Step: 6450... Loss: 1.3528... Val Loss: 1.4581\nEpoch: 14/20... Step: 6460... Loss: 1.3675... Val Loss: 1.4545\nEpoch: 14/20... Step: 6470... Loss: 1.3550... Val Loss: 1.4515\nEpoch: 14/20... Step: 6480... Loss: 1.3467... Val Loss: 1.4527\nEpoch: 14/20... Step: 6490... Loss: 1.3420... Val Loss: 1.4598\nEpoch: 14/20... Step: 6500... Loss: 1.3678... Val Loss: 1.4547\nEpoch: 14/20... Step: 6510... Loss: 1.3818... Val Loss: 1.4555\nEpoch: 14/20... Step: 6520... Loss: 1.3895... Val Loss: 1.4593\nEpoch: 14/20... Step: 6530... Loss: 1.3953... Val Loss: 1.4526\nEpoch: 14/20... Step: 6540... Loss: 1.3470... Val Loss: 1.4559\nEpoch: 14/20... Step: 6550... Loss: 1.3793... Val Loss: 1.4607\nEpoch: 14/20... Step: 6560... Loss: 1.3611... Val Loss: 1.4539\nEpoch: 15/20... Step: 6570... Loss: 1.3890... Val Loss: 1.4584\nEpoch: 15/20... Step: 6580... Loss: 1.3557... Val Loss: 1.4528\nEpoch: 15/20... Step: 6590... Loss: 1.3905... Val Loss: 1.4547\nEpoch: 15/20... Step: 6600... Loss: 1.3822... Val Loss: 1.4553\nEpoch: 15/20... Step: 6610... Loss: 1.3704... Val Loss: 1.4510\nEpoch: 15/20... Step: 6620... Loss: 1.3976... Val Loss: 1.4539\nEpoch: 15/20... Step: 6630... Loss: 1.3906... Val Loss: 1.4573\nEpoch: 15/20... Step: 6640... Loss: 1.3738... Val Loss: 1.4544\nEpoch: 15/20... Step: 6650... Loss: 1.3978... Val Loss: 1.4566\nEpoch: 15/20... Step: 6660... Loss: 1.3707... Val Loss: 1.4551\nEpoch: 15/20... Step: 6670... Loss: 1.3535... Val Loss: 1.4513\nEpoch: 15/20... Step: 6680... Loss: 1.3942... Val Loss: 1.4567\nEpoch: 15/20... Step: 6690... Loss: 1.3873... Val Loss: 1.4502\nEpoch: 15/20... Step: 6700... Loss: 1.3948... Val Loss: 1.4539\nEpoch: 15/20... Step: 6710... Loss: 1.3429... Val Loss: 1.4515\nEpoch: 15/20... Step: 6720... Loss: 1.3578... Val Loss: 1.4530\nEpoch: 15/20... Step: 6730... Loss: 1.3655... Val Loss: 1.4511\nEpoch: 15/20... Step: 6740... Loss: 1.3800... Val Loss: 1.4506\nEpoch: 15/20... Step: 6750... Loss: 1.3481... Val Loss: 1.4512\nEpoch: 15/20... Step: 6760... Loss: 1.3801... Val Loss: 1.4516\nEpoch: 15/20... Step: 6770... Loss: 1.3700... Val Loss: 1.4550\nEpoch: 15/20... Step: 6780... Loss: 1.3388... Val Loss: 1.4538\nEpoch: 15/20... Step: 6790... Loss: 1.3404... Val Loss: 1.4551\nEpoch: 15/20... Step: 6800... Loss: 1.3486... Val Loss: 1.4516\nEpoch: 15/20... Step: 6810... Loss: 1.3500... Val Loss: 1.4493\nEpoch: 15/20... Step: 6820... Loss: 1.3583... Val Loss: 1.4489\nEpoch: 15/20... Step: 6830... Loss: 1.3780... Val Loss: 1.4526\nEpoch: 15/20... Step: 6840... Loss: 1.3731... Val Loss: 1.4492\nEpoch: 15/20... Step: 6850... Loss: 1.3518... Val Loss: 1.4478\nEpoch: 15/20... Step: 6860... Loss: 1.3698... Val Loss: 1.4487\nEpoch: 15/20... Step: 6870... Loss: 1.3567... Val Loss: 1.4483\nEpoch: 15/20... Step: 6880... Loss: 1.3726... Val Loss: 1.4494\nEpoch: 15/20... Step: 6890... Loss: 1.3546... Val Loss: 1.4486\nEpoch: 15/20... Step: 6900... Loss: 1.3781... Val Loss: 1.4465\nEpoch: 15/20... Step: 6910... Loss: 1.3463... Val Loss: 1.4504\nEpoch: 15/20... Step: 6920... Loss: 1.3460... Val Loss: 1.4518\nEpoch: 15/20... Step: 6930... Loss: 1.3529... Val Loss: 1.4481\nEpoch: 15/20... Step: 6940... Loss: 1.3645... Val Loss: 1.4487\nEpoch: 15/20... Step: 6950... Loss: 1.3416... Val Loss: 1.4488\nEpoch: 15/20... Step: 6960... Loss: 1.3577... Val Loss: 1.4502\nEpoch: 15/20... Step: 6970... Loss: 1.3478... Val Loss: 1.4535\nEpoch: 15/20... Step: 6980... Loss: 1.3913... Val Loss: 1.4510\nEpoch: 15/20... Step: 6990... Loss: 1.3836... Val Loss: 1.4514\nEpoch: 15/20... Step: 7000... Loss: 1.3609... Val Loss: 1.4515\nEpoch: 15/20... Step: 7010... Loss: 1.3536... Val Loss: 1.4510\nEpoch: 15/20... Step: 7020... Loss: 1.3845... Val Loss: 1.4535\nEpoch: 15/20... Step: 7030... Loss: 1.3615... Val Loss: 1.4470\nEpoch: 16/20... Step: 7040... Loss: 1.3532... Val Loss: 1.4543\nEpoch: 16/20... Step: 7050... Loss: 1.3499... Val Loss: 1.4481\nEpoch: 16/20... Step: 7060... Loss: 1.3332... Val Loss: 1.4482\nEpoch: 16/20... Step: 7070... Loss: 1.3841... Val Loss: 1.4514\nEpoch: 16/20... Step: 7080... Loss: 1.3761... Val Loss: 1.4470\nEpoch: 16/20... Step: 7090... Loss: 1.3646... Val Loss: 1.4524\nEpoch: 16/20... Step: 7100... Loss: 1.3636... Val Loss: 1.4512\nEpoch: 16/20... Step: 7110... Loss: 1.3583... Val Loss: 1.4518\nEpoch: 16/20... Step: 7120... Loss: 1.3570... Val Loss: 1.4521\nEpoch: 16/20... Step: 7130... Loss: 1.3541... Val Loss: 1.4533\nEpoch: 16/20... Step: 7140... Loss: 1.3412... Val Loss: 1.4477\nEpoch: 16/20... Step: 7150... Loss: 1.3612... Val Loss: 1.4512\nEpoch: 16/20... Step: 7160... Loss: 1.3507... Val Loss: 1.4486\nEpoch: 16/20... Step: 7170... Loss: 1.3659... Val Loss: 1.4491\nEpoch: 16/20... Step: 7180... Loss: 1.3519... Val Loss: 1.4528\nEpoch: 16/20... Step: 7190... Loss: 1.3605... Val Loss: 1.4538\nEpoch: 16/20... Step: 7200... Loss: 1.3971... Val Loss: 1.4456\nEpoch: 16/20... Step: 7210... Loss: 1.3513... Val Loss: 1.4473\nEpoch: 16/20... Step: 7220... Loss: 1.3679... Val Loss: 1.4474\nEpoch: 16/20... Step: 7230... Loss: 1.3887... Val Loss: 1.4503\nEpoch: 16/20... Step: 7240... Loss: 1.3593... Val Loss: 1.4489\nEpoch: 16/20... Step: 7250... Loss: 1.3605... Val Loss: 1.4514\nEpoch: 16/20... Step: 7260... Loss: 1.3577... Val Loss: 1.4475\nEpoch: 16/20... Step: 7270... Loss: 1.3319... Val Loss: 1.4459\nEpoch: 16/20... Step: 7280... Loss: 1.3352... Val Loss: 1.4472\nEpoch: 16/20... Step: 7290... Loss: 1.3673... Val Loss: 1.4439\nEpoch: 16/20... Step: 7300... Loss: 1.3370... Val Loss: 1.4499\nEpoch: 16/20... Step: 7310... Loss: 1.3415... Val Loss: 1.4447\nEpoch: 16/20... Step: 7320... Loss: 1.3361... Val Loss: 1.4437\nEpoch: 16/20... Step: 7330... Loss: 1.3746... Val Loss: 1.4441\nEpoch: 16/20... Step: 7340... Loss: 1.3445... Val Loss: 1.4459\nEpoch: 16/20... Step: 7350... Loss: 1.3519... Val Loss: 1.4464\nEpoch: 16/20... Step: 7360... Loss: 1.3325... Val Loss: 1.4448\nEpoch: 16/20... Step: 7370... Loss: 1.3530... Val Loss: 1.4413\nEpoch: 16/20... Step: 7380... Loss: 1.3620... Val Loss: 1.4456\nEpoch: 16/20... Step: 7390... Loss: 1.3503... Val Loss: 1.4458\nEpoch: 16/20... Step: 7400... Loss: 1.3441... Val Loss: 1.4449\nEpoch: 16/20... Step: 7410... Loss: 1.3807... Val Loss: 1.4440\nEpoch: 16/20... Step: 7420... Loss: 1.3563... Val Loss: 1.4430\nEpoch: 16/20... Step: 7430... Loss: 1.3617... Val Loss: 1.4460\nEpoch: 16/20... Step: 7440... Loss: 1.3603... Val Loss: 1.4520\nEpoch: 16/20... Step: 7450... Loss: 1.3987... Val Loss: 1.4459\nEpoch: 16/20... Step: 7460... Loss: 1.3573... Val Loss: 1.4476\nEpoch: 16/20... Step: 7470... Loss: 1.3304... Val Loss: 1.4475\nEpoch: 16/20... Step: 7480... Loss: 1.3332... Val Loss: 1.4479\nEpoch: 16/20... Step: 7490... Loss: 1.3578... Val Loss: 1.4478\nEpoch: 16/20... Step: 7500... Loss: 1.3439... Val Loss: 1.4450\nEpoch: 17/20... Step: 7510... Loss: 1.3457... Val Loss: 1.4515\nEpoch: 17/20... Step: 7520... Loss: 1.3725... Val Loss: 1.4470\nEpoch: 17/20... Step: 7530... Loss: 1.3743... Val Loss: 1.4490\nEpoch: 17/20... Step: 7540... Loss: 1.3265... Val Loss: 1.4500\nEpoch: 17/20... Step: 7550... Loss: 1.3573... Val Loss: 1.4420\nEpoch: 17/20... Step: 7560... Loss: 1.3260... Val Loss: 1.4449\nEpoch: 17/20... Step: 7570... Loss: 1.3585... Val Loss: 1.4490\nEpoch: 17/20... Step: 7580... Loss: 1.3521... Val Loss: 1.4523\nEpoch: 17/20... Step: 7590... Loss: 1.3321... Val Loss: 1.4487\nEpoch: 17/20... Step: 7600... Loss: 1.3301... Val Loss: 1.4456\nEpoch: 17/20... Step: 7610... Loss: 1.3370... Val Loss: 1.4455\nEpoch: 17/20... Step: 7620... Loss: 1.3394... Val Loss: 1.4466\nEpoch: 17/20... Step: 7630... Loss: 1.3507... Val Loss: 1.4452\nEpoch: 17/20... Step: 7640... Loss: 1.3569... Val Loss: 1.4461\nEpoch: 17/20... Step: 7650... Loss: 1.3615... Val Loss: 1.4501\nEpoch: 17/20... Step: 7660... Loss: 1.3790... Val Loss: 1.4450\nEpoch: 17/20... Step: 7670... Loss: 1.3625... Val Loss: 1.4408\nEpoch: 17/20... Step: 7680... Loss: 1.3585... Val Loss: 1.4431\nEpoch: 17/20... Step: 7690... Loss: 1.3535... Val Loss: 1.4402\nEpoch: 17/20... Step: 7700... Loss: 1.3418... Val Loss: 1.4494\nEpoch: 17/20... Step: 7710... Loss: 1.3496... Val Loss: 1.4451\nEpoch: 17/20... Step: 7720... Loss: 1.3318... Val Loss: 1.4454\nEpoch: 17/20... Step: 7730... Loss: 1.3324... Val Loss: 1.4452\nEpoch: 17/20... Step: 7740... Loss: 1.3375... Val Loss: 1.4411\nEpoch: 17/20... Step: 7750... Loss: 1.3389... Val Loss: 1.4443\nEpoch: 17/20... Step: 7760... Loss: 1.3500... Val Loss: 1.4393\nEpoch: 17/20... Step: 7770... Loss: 1.3246... Val Loss: 1.4486\nEpoch: 17/20... Step: 7780... Loss: 1.3408... Val Loss: 1.4429\nEpoch: 17/20... Step: 7790... Loss: 1.3406... Val Loss: 1.4405\nEpoch: 17/20... Step: 7800... Loss: 1.3619... Val Loss: 1.4414\nEpoch: 17/20... Step: 7810... Loss: 1.3532... Val Loss: 1.4413\nEpoch: 17/20... Step: 7820... Loss: 1.3504... Val Loss: 1.4455\nEpoch: 17/20... Step: 7830... Loss: 1.3490... Val Loss: 1.4434\nEpoch: 17/20... Step: 7840... Loss: 1.3322... Val Loss: 1.4383\nEpoch: 17/20... Step: 7850... Loss: 1.3556... Val Loss: 1.4416\nEpoch: 17/20... Step: 7860... Loss: 1.3465... Val Loss: 1.4413\nEpoch: 17/20... Step: 7870... Loss: 1.3660... Val Loss: 1.4463\nEpoch: 17/20... Step: 7880... Loss: 1.3381... Val Loss: 1.4404\nEpoch: 17/20... Step: 7890... Loss: 1.3486... Val Loss: 1.4398\nEpoch: 17/20... Step: 7900... Loss: 1.3201... Val Loss: 1.4415\nEpoch: 17/20... Step: 7910... Loss: 1.3591... Val Loss: 1.4481\nEpoch: 17/20... Step: 7920... Loss: 1.3435... Val Loss: 1.4411\nEpoch: 17/20... Step: 7930... Loss: 1.3560... Val Loss: 1.4427\nEpoch: 17/20... Step: 7940... Loss: 1.3457... Val Loss: 1.4431\nEpoch: 17/20... Step: 7950... Loss: 1.3483... Val Loss: 1.4411\nEpoch: 17/20... Step: 7960... Loss: 1.3379... Val Loss: 1.4448\nEpoch: 17/20... Step: 7970... Loss: 1.3434... Val Loss: 1.4428\nEpoch: 18/20... Step: 7980... Loss: 1.3285... Val Loss: 1.4492\nEpoch: 18/20... Step: 7990... Loss: 1.3551... Val Loss: 1.4463\nEpoch: 18/20... Step: 8000... Loss: 1.3639... Val Loss: 1.4427\nEpoch: 18/20... Step: 8010... Loss: 1.3436... Val Loss: 1.4487\nEpoch: 18/20... Step: 8020... Loss: 1.3347... Val Loss: 1.4379\nEpoch: 18/20... Step: 8030... Loss: 1.3545... Val Loss: 1.4438\nEpoch: 18/20... Step: 8040... Loss: 1.3661... Val Loss: 1.4427\nEpoch: 18/20... Step: 8050... Loss: 1.3145... Val Loss: 1.4473\nEpoch: 18/20... Step: 8060... Loss: 1.3504... Val Loss: 1.4444\nEpoch: 18/20... Step: 8070... Loss: 1.3285... Val Loss: 1.4421\nEpoch: 18/20... Step: 8080... Loss: 1.3136... Val Loss: 1.4440\nEpoch: 18/20... Step: 8090... Loss: 1.3377... Val Loss: 1.4429\nEpoch: 18/20... Step: 8100... Loss: 1.3365... Val Loss: 1.4419\nEpoch: 18/20... Step: 8110... Loss: 1.3448... Val Loss: 1.4394\nEpoch: 18/20... Step: 8120... Loss: 1.3394... Val Loss: 1.4460\nEpoch: 18/20... Step: 8130... Loss: 1.3565... Val Loss: 1.4442\nEpoch: 18/20... Step: 8140... Loss: 1.3524... Val Loss: 1.4381\nEpoch: 18/20... Step: 8150... Loss: 1.3056... Val Loss: 1.4392\nEpoch: 18/20... Step: 8160... Loss: 1.3349... Val Loss: 1.4400\nEpoch: 18/20... Step: 8170... Loss: 1.3470... Val Loss: 1.4463\nEpoch: 18/20... Step: 8180... Loss: 1.3237... Val Loss: 1.4395\nEpoch: 18/20... Step: 8190... Loss: 1.3301... Val Loss: 1.4433\nEpoch: 18/20... Step: 8200... Loss: 1.3342... Val Loss: 1.4419\nEpoch: 18/20... Step: 8210... Loss: 1.3332... Val Loss: 1.4413\nEpoch: 18/20... Step: 8220... Loss: 1.3567... Val Loss: 1.4385\nEpoch: 18/20... Step: 8230... Loss: 1.3308... Val Loss: 1.4376\nEpoch: 18/20... Step: 8240... Loss: 1.3587... Val Loss: 1.4386\nEpoch: 18/20... Step: 8250... Loss: 1.3541... Val Loss: 1.4376\nEpoch: 18/20... Step: 8260... Loss: 1.3291... Val Loss: 1.4391\nEpoch: 18/20... Step: 8270... Loss: 1.3122... Val Loss: 1.4370\nEpoch: 18/20... Step: 8280... Loss: 1.3301... Val Loss: 1.4365\nEpoch: 18/20... Step: 8290... Loss: 1.3406... Val Loss: 1.4413\nEpoch: 18/20... Step: 8300... Loss: 1.3156... Val Loss: 1.4404\nEpoch: 18/20... Step: 8310... Loss: 1.3283... Val Loss: 1.4344\nEpoch: 18/20... Step: 8320... Loss: 1.3337... Val Loss: 1.4388\nEpoch: 18/20... Step: 8330... Loss: 1.3565... Val Loss: 1.4361\nEpoch: 18/20... Step: 8340... Loss: 1.3360... Val Loss: 1.4409\nEpoch: 18/20... Step: 8350... Loss: 1.3595... Val Loss: 1.4398\nEpoch: 18/20... Step: 8360... Loss: 1.3046... Val Loss: 1.4372\nEpoch: 18/20... Step: 8370... Loss: 1.3183... Val Loss: 1.4385\nEpoch: 18/20... Step: 8380... Loss: 1.3456... Val Loss: 1.4436\nEpoch: 18/20... Step: 8390... Loss: 1.3411... Val Loss: 1.4402\nEpoch: 18/20... Step: 8400... Loss: 1.3177... Val Loss: 1.4385\nEpoch: 18/20... Step: 8410... Loss: 1.3428... Val Loss: 1.4435\nEpoch: 18/20... Step: 8420... Loss: 1.3153... Val Loss: 1.4397\nEpoch: 18/20... Step: 8430... Loss: 1.3405... Val Loss: 1.4413\nEpoch: 18/20... Step: 8440... Loss: 1.3345... Val Loss: 1.4387\nEpoch: 19/20... Step: 8450... Loss: 1.3535... Val Loss: 1.4428\nEpoch: 19/20... Step: 8460... Loss: 1.3278... Val Loss: 1.4418\nEpoch: 19/20... Step: 8470... Loss: 1.3302... Val Loss: 1.4401\nEpoch: 19/20... Step: 8480... Loss: 1.3456... Val Loss: 1.4458\nEpoch: 19/20... Step: 8490... Loss: 1.3248... Val Loss: 1.4341\nEpoch: 19/20... Step: 8500... Loss: 1.3276... Val Loss: 1.4381\nEpoch: 19/20... Step: 8510... Loss: 1.3131... Val Loss: 1.4378\nEpoch: 19/20... Step: 8520... Loss: 1.3303... Val Loss: 1.4444\nEpoch: 19/20... Step: 8530... Loss: 1.3376... Val Loss: 1.4414\nEpoch: 19/20... Step: 8540... Loss: 1.3512... Val Loss: 1.4391\nEpoch: 19/20... Step: 8550... Loss: 1.3148... Val Loss: 1.4393\nEpoch: 19/20... Step: 8560... Loss: 1.3447... Val Loss: 1.4375\nEpoch: 19/20... Step: 8570... Loss: 1.3384... Val Loss: 1.4414\nEpoch: 19/20... Step: 8580... Loss: 1.3226... Val Loss: 1.4384\nEpoch: 19/20... Step: 8590... Loss: 1.3133... Val Loss: 1.4430\nEpoch: 19/20... Step: 8600... Loss: 1.3452... Val Loss: 1.4390\nEpoch: 19/20... Step: 8610... Loss: 1.3285... Val Loss: 1.4371\nEpoch: 19/20... Step: 8620... Loss: 1.3381... Val Loss: 1.4335\nEpoch: 19/20... Step: 8630... Loss: 1.3369... Val Loss: 1.4373\nEpoch: 19/20... Step: 8640... Loss: 1.3329... Val Loss: 1.4468\nEpoch: 19/20... Step: 8650... Loss: 1.3493... Val Loss: 1.4356\nEpoch: 19/20... Step: 8660... Loss: 1.3609... Val Loss: 1.4380\nEpoch: 19/20... Step: 8670... Loss: 1.3447... Val Loss: 1.4380\nEpoch: 19/20... Step: 8680... Loss: 1.3507... Val Loss: 1.4361\nEpoch: 19/20... Step: 8690... Loss: 1.3299... Val Loss: 1.4376\nEpoch: 19/20... Step: 8700... Loss: 1.3096... Val Loss: 1.4323\nEpoch: 19/20... Step: 8710... Loss: 1.3234... Val Loss: 1.4374\nEpoch: 19/20... Step: 8720... Loss: 1.3277... Val Loss: 1.4358\nEpoch: 19/20... Step: 8730... Loss: 1.3340... Val Loss: 1.4348\nEpoch: 19/20... Step: 8740... Loss: 1.3361... Val Loss: 1.4378\nEpoch: 19/20... Step: 8750... Loss: 1.3161... Val Loss: 1.4342\nEpoch: 19/20... Step: 8760... Loss: 1.3282... Val Loss: 1.4384\nEpoch: 19/20... Step: 8770... Loss: 1.3530... Val Loss: 1.4396\nEpoch: 19/20... Step: 8780... Loss: 1.3396... Val Loss: 1.4335\nEpoch: 19/20... Step: 8790... Loss: 1.3203... Val Loss: 1.4386\nEpoch: 19/20... Step: 8800... Loss: 1.2959... Val Loss: 1.4358\nEpoch: 19/20... Step: 8810... Loss: 1.3423... Val Loss: 1.4388\nEpoch: 19/20... Step: 8820... Loss: 1.3240... Val Loss: 1.4347\nEpoch: 19/20... Step: 8830... Loss: 1.3162... Val Loss: 1.4372\nEpoch: 19/20... Step: 8840... Loss: 1.3226... Val Loss: 1.4367\nEpoch: 19/20... Step: 8850... Loss: 1.3312... Val Loss: 1.4456\nEpoch: 19/20... Step: 8860... Loss: 1.3404... Val Loss: 1.4365\nEpoch: 19/20... Step: 8870... Loss: 1.3417... Val Loss: 1.4355\nEpoch: 19/20... Step: 8880... Loss: 1.3311... Val Loss: 1.4393\nEpoch: 19/20... Step: 8890... Loss: 1.3035... Val Loss: 1.4363\nEpoch: 19/20... Step: 8900... Loss: 1.3420... Val Loss: 1.4435\nEpoch: 19/20... Step: 8910... Loss: 1.3231... Val Loss: 1.4359\nEpoch: 20/20... Step: 8920... Loss: 1.3210... Val Loss: 1.4397\nEpoch: 20/20... Step: 8930... Loss: 1.3387... Val Loss: 1.4399\nEpoch: 20/20... Step: 8940... Loss: 1.3362... Val Loss: 1.4359\nEpoch: 20/20... Step: 8950... Loss: 1.3205... Val Loss: 1.4456\nEpoch: 20/20... Step: 8960... Loss: 1.3639... Val Loss: 1.4343\nEpoch: 20/20... Step: 8970... Loss: 1.3346... Val Loss: 1.4382\nEpoch: 20/20... Step: 8980... Loss: 1.2964... Val Loss: 1.4366\nEpoch: 20/20... Step: 8990... Loss: 1.3219... Val Loss: 1.4430\nEpoch: 20/20... Step: 9000... Loss: 1.3263... Val Loss: 1.4390\nEpoch: 20/20... Step: 9010... Loss: 1.3236... Val Loss: 1.4370\nEpoch: 20/20... Step: 9020... Loss: 1.3128... Val Loss: 1.4400\nEpoch: 20/20... Step: 9030... Loss: 1.3202... Val Loss: 1.4342\nEpoch: 20/20... Step: 9040... Loss: 1.3210... Val Loss: 1.4388\nEpoch: 20/20... Step: 9050... Loss: 1.3275... Val Loss: 1.4361\nEpoch: 20/20... Step: 9060... Loss: 1.3358... Val Loss: 1.4422\nEpoch: 20/20... Step: 9070... Loss: 1.3339... Val Loss: 1.4369\nEpoch: 20/20... Step: 9080... Loss: 1.3108... Val Loss: 1.4356\nEpoch: 20/20... Step: 9090... Loss: 1.3211... Val Loss: 1.4312\nEpoch: 20/20... Step: 9100... Loss: 1.3176... Val Loss: 1.4339\nEpoch: 20/20... Step: 9110... Loss: 1.3344... Val Loss: 1.4399\nEpoch: 20/20... Step: 9120... Loss: 1.3019... Val Loss: 1.4351\nEpoch: 20/20... Step: 9130... Loss: 1.3238... Val Loss: 1.4368\nEpoch: 20/20... Step: 9140... Loss: 1.3321... Val Loss: 1.4348\nEpoch: 20/20... Step: 9150... Loss: 1.3228... Val Loss: 1.4355\nEpoch: 20/20... Step: 9160... Loss: 1.3315... Val Loss: 1.4329\nEpoch: 20/20... Step: 9170... Loss: 1.3224... Val Loss: 1.4338\nEpoch: 20/20... Step: 9180... Loss: 1.3258... Val Loss: 1.4332\nEpoch: 20/20... Step: 9190... Loss: 1.3390... Val Loss: 1.4369\nEpoch: 20/20... Step: 9200... Loss: 1.3324... Val Loss: 1.4335\nEpoch: 20/20... Step: 9210... Loss: 1.3183... Val Loss: 1.4334\nEpoch: 20/20... Step: 9220... Loss: 1.3100... Val Loss: 1.4320\nEpoch: 20/20... Step: 9230... Loss: 1.3035... Val Loss: 1.4338\nEpoch: 20/20... Step: 9240... Loss: 1.3537... Val Loss: 1.4427\nEpoch: 20/20... Step: 9250... Loss: 1.3257... Val Loss: 1.4317\nEpoch: 20/20... Step: 9260... Loss: 1.2921... Val Loss: 1.4390\nEpoch: 20/20... Step: 9270... Loss: 1.3124... Val Loss: 1.4356\nEpoch: 20/20... Step: 9280... Loss: 1.3325... Val Loss: 1.4338\nEpoch: 20/20... Step: 9290... Loss: 1.3132... Val Loss: 1.4381\nEpoch: 20/20... Step: 9300... Loss: 1.3198... Val Loss: 1.4350\nEpoch: 20/20... Step: 9310... Loss: 1.3131... Val Loss: 1.4358\nEpoch: 20/20... Step: 9320... Loss: 1.3168... Val Loss: 1.4445\nEpoch: 20/20... Step: 9330... Loss: 1.3201... Val Loss: 1.4358\nEpoch: 20/20... Step: 9340... Loss: 1.3193... Val Loss: 1.4332\nEpoch: 20/20... Step: 9350... Loss: 1.3168... Val Loss: 1.4384\nEpoch: 20/20... Step: 9360... Loss: 1.3156... Val Loss: 1.4360\nEpoch: 20/20... Step: 9370... Loss: 1.3511... Val Loss: 1.4378\nEpoch: 20/20... Step: 9380... Loss: 1.3263... Val Loss: 1.4334\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint\n"
      ],
      "metadata": {
        "id": "ZfZxvNoDceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"rnn_x_epoch.net\"\n",
        "\n",
        "checkpoint = {\n",
        "    \"n_hidden\": net.n_hidden,\n",
        "    \"n_layers\": net.n_layers,\n",
        "    \"state_dict\": net.state_dict(),\n",
        "    \"tokens\": net.chars,\n",
        "}\n",
        "\n",
        "with open(model_name, \"wb\") as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "metadata": {
        "id": "q6RXl5VAceEm",
        "execution": {
          "iopub.status.busy": "2024-02-03T19:40:09.036362Z",
          "iopub.execute_input": "2024-02-03T19:40:09.036797Z",
          "iopub.status.idle": "2024-02-03T19:40:09.211112Z",
          "shell.execute_reply.started": "2024-02-03T19:40:09.03677Z",
          "shell.execute_reply": "2024-02-03T19:40:09.210107Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Making Prediction"
      ],
      "metadata": {
        "id": "K2sJhx5iceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ],
      "metadata": {
        "id": "QEIRW_B2ceEm",
        "execution": {
          "iopub.status.busy": "2024-02-03T19:40:09.212293Z",
          "iopub.execute_input": "2024-02-03T19:40:09.212628Z",
          "iopub.status.idle": "2024-02-03T19:40:09.222128Z",
          "shell.execute_reply.started": "2024-02-03T19:40:09.212602Z",
          "shell.execute_reply": "2024-02-03T19:40:09.221176Z"
        },
        "trusted": true
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Priming and Text Generating"
      ],
      "metadata": {
        "id": "OG38j3gQceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime=\"Начало\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ],
      "metadata": {
        "id": "P9vpB5gRceEm",
        "execution": {
          "iopub.status.busy": "2024-02-03T19:40:09.223547Z",
          "iopub.execute_input": "2024-02-03T19:40:09.223885Z",
          "iopub.status.idle": "2024-02-03T19:40:09.231946Z",
          "shell.execute_reply.started": "2024-02-03T19:40:09.223853Z",
          "shell.execute_reply": "2024-02-03T19:40:09.231173Z"
        },
        "trusted": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime=\"Добрый день\", top_k=2))"
      ],
      "metadata": {
        "id": "BqmFA9eEceEm",
        "execution": {
          "iopub.status.busy": "2024-02-03T19:40:09.233043Z",
          "iopub.execute_input": "2024-02-03T19:40:09.233783Z",
          "iopub.status.idle": "2024-02-03T19:40:09.906816Z",
          "shell.execute_reply.started": "2024-02-03T19:40:09.233751Z",
          "shell.execute_reply": "2024-02-03T19:40:09.905895Z"
        },
        "trusted": true,
        "outputId": "20892758-ebb8-4f79-818c-a638ecb33132"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Добрый день по полу по своему получению своей силы. В тело старые страны пространства на столе поставили с подоконника в подводной поляне. В том жилище он поднял его, прочел и подошел к светлому стеклу, поднимал его свою силу и поднялся на пол и скрывался на пол и пошел в свое поле. Но он стал принимать в ней не столько свои страны и прочие привыкли к своим солнцем.\n\n— Ты стал, товарищ Чепурный? — спросил он. — Ты прочь с нами, а ты стал старым и после своего солдата.\n\n— Ну как ты сколько ты, ты подождешь? — сказал Сербинов и спросил его про себя:\n\n— А я с ними полюбил — только в светлом поле начнется. А вы не помните, какое тело продолжается из своих пространств и нашего состава нашей привязанности к ней и получить, — скажи мне по сторону положения, а не в темноте станции.\n\n— Нет, я не приду на свете!\n\n— Ну как же, товарищ капитан, — сказал он. — Ты скоро умру не старуху, а ты не помнишь меня, ты не привыкла бы привыкну к сердце и придется страшно. Но ты не помнишь меня, ты скажи мне по нему, что\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the generated text we can draw the following conclusions.\n",
        "\n",
        "First, when using CharRNN it is rather hard to achieve meaningfulness of the text.\n",
        "In most cases we failed to achieve meaningfulness, but sometimes words are formed correctly,\n",
        "for example: \"на столе поставили\", \"в том жилище он поднял его, прочел и подошел к светлому стеклу\".\n",
        "\n",
        "In addition, we managed to achieve stylistically good generation. Platonov's texts have always possessed\n",
        "\"arbitrariness in the combination of words,\" as many critics have noted. This is what we see: \"тело продолжается из своих\n",
        "пространств\", \"состава нашей привязанности к ней\".\n",
        "\n",
        "In addition, we see that the neural network has a good grasp of the punctuation of the Russian language,\n",
        "which is evident in the grammatically correct dialogs.\n",
        "\n",
        "With morphemics things are worse, the text contains many words that are constructed incorrectly,\n",
        "such as \"ты скоро умрУ\"."
      ],
      "metadata": {
        "id": "s0Wfz2zBiWpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a checkpoint"
      ],
      "metadata": {
        "id": "942mjdQHceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we have loaded in a model\n",
        "with open(\"rnn_x_epoch.net\", \"rb\") as f:\n",
        "    checkpoint = torch.load(f)\n",
        "\n",
        "loaded = CharRNN(\n",
        "    checkpoint[\"tokens\"],\n",
        "    n_hidden=checkpoint[\"n_hidden\"],\n",
        "    n_layers=checkpoint[\"n_layers\"],\n",
        ")\n",
        "loaded.load_state_dict(checkpoint[\"state_dict\"])"
      ],
      "metadata": {
        "id": "Xt9ldUuSceEm",
        "execution": {
          "iopub.status.busy": "2024-02-03T19:40:09.908208Z",
          "iopub.execute_input": "2024-02-03T19:40:09.908794Z",
          "iopub.status.idle": "2024-02-03T19:40:10.2172Z",
          "shell.execute_reply.started": "2024-02-03T19:40:09.908757Z",
          "shell.execute_reply": "2024-02-03T19:40:10.216234Z"
        },
        "trusted": true,
        "outputId": "f53e0378-9d64-4a7b-baeb-40f110b25f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"Погода была\"))"
      ],
      "metadata": {
        "id": "Ut6R3zDcceEm",
        "execution": {
          "iopub.status.busy": "2024-02-03T19:40:10.218238Z",
          "iopub.execute_input": "2024-02-03T19:40:10.218526Z",
          "iopub.status.idle": "2024-02-03T19:40:11.538821Z",
          "shell.execute_reply.started": "2024-02-03T19:40:10.218494Z",
          "shell.execute_reply": "2024-02-03T19:40:11.537832Z"
        },
        "trusted": true,
        "outputId": "74c9c5dc-f075-4ae3-c40e-f3f6582a3006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Погода была в самом деле и на поле, не превращающиеся к ней.\n",
            "\n",
            "— А ты в культуре составил, а ты сейчас? А ты страшно теперь, а не под конец…\n",
            "\n",
            "В сапог он никогда не успел подняться; он посмотрел на мальчика, потому что неизвестно было в пустоте небольшой девичьих ламп, потому что водой в степе стало скрытое плодное стекло. По своему случаю полезно искали остаток. Но нет того, что страдало и не может стать правыми словами. В стороне немцев, который не знал в своей пищевой судьбе, а по всей дороге было скончаться не при старых и последнего чужая. На всю слободу на две трудные вещи паровозных стоит стояли трамвае, по небольшому тому и стану не будут справлять искусства. Но несколько старик стал думать о ней и приказал себя вскоре обратить на свою страсть и на пустой петухов, искать ее и скупо и неподвижно. Прохожий красноармеец пришел к старости, все рассеянно склонился к своему воздуху.\n",
            "\n",
            "— Ну дочь, — ответил Соплян и стал несли старое действие.\n",
            "\n",
            "Перин поступил подпользоваться на степной дом, и под поселке под короткими полями вставали с него воду.\n",
            "\n",
            "Выше остались в одной пазовой советской силе своей страны; все солнце было под старицой и переводить коня в тот странник и полником представления и страха.\n",
            "\n",
            "На противоположные сумерки они умерли, и в печке старуха все более стала написать петухами.\n",
            "\n",
            "Но под сором времен он следит за открытым коров и стал перестанет из пастух и на стене, и стала она выдавалась с подобие своего собственного торжества. Но она видела в ней, насколько он придерживает его и начинает плакать от нее, чтобы начать его сознание.\n",
            "\n",
            "На домашнем столе сидел один над собой. Она прошла и обернулась на старика и поднимала себе свет.\n",
            "\n",
            "Но при нем всю жизнь приходилось понимать его, но не боялся старый полен просто и получали против него на нем на несколько половых рук. Общее совершенно пришлось стало на прежнем поляте. В то время он никогда не понял его и отошел отстоять всех.\n",
            "\n",
            "Не спали, они никто не воображали, как они получили, и она не знала. Ведь не получила все воздух \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9SUd8vdhdwL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}